<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
<link href="http://gmpg.org/xfn/11" rel="profile">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="content-type" content="text/html; charset=utf-8">

<!-- Enable responsiveness on mobile devices-->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

<title>
    
    Xavier Initialization &middot; Manas George
    
</title>

<!-- CSS -->
<link rel="stylesheet" href="/public/css/poole.css">
<link rel="stylesheet" href="/public/css/syntax.css">
<link rel="stylesheet" href="/public/css/hyde.css">
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

<!-- Icons -->
<link rel="shortcut icon" href="/public/favicon.ico">

<!-- RSS -->
<link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111529460-1"></script>
<script>
window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-111529460-1');
    </script>

</head>


    <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
        },
        jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
        extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js",
                     "MathMenu.js","MathZoom.js","AssistiveMML.js"],
        TeX: {
            extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
            equationNumbers: {
                autoNumber: "AMS"
            },
            Macros: {
                bra: ["{\\mathinner{\\langle{#1}|}}",1],
                ket: ["{\\mathinner{|{#1}\\rangle}}",1],
                braket: ["{\\mathinner{\\langle{#1}\\rangle}}",1],
                Bra: ["{\\left\\langle#1\\right|}",1],
                Ket: ["{\\left|#1\\right\\rangle}",1]
            }
        }
    });
    </script>



  <body class="theme-base-08">

    <div class="sidebar">
<div class="container sidebar-sticky">
    <div class="sidebar-about">
        <h1>
            <a href="/">
                <div class="profile-picture">
                    <img src="/assets/face.png" width="200"></img>
                </div>
                <!-- Manas George -->
            </a>
        </h1>
        <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
        

        <a class="sidebar-nav-item" href="/">Blog</a>
        <a class="sidebar-nav-item" href="/resume">Resume</a>
        <a class="sidebar-nav-item" href="https://www.github.com/ManasGeorge">GitHub</a>
    </nav>
</div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Xavier Initialization</h1>
  <span class="post-date">21 Dec 2017</span>
  <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#vanishing-gradients">Vanishing Gradients</a></li>
<li class="toc-entry toc-h2"><a href="#glorot-and-bengio">Glorot and Bengio</a></li>
<li class="toc-entry toc-h2"><a href="#notation-and-assumptions">Notation and Assumptions</a></li>
<li class="toc-entry toc-h2"><a href="#forward-pass">Forward pass</a></li>
<li class="toc-entry toc-h2"><a href="#backwards-pass">Backwards pass</a></li>
<li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a>
<ul>
<li class="toc-entry toc-h3"><a href="#logistic-activation">Logistic Activation</a></li>
<li class="toc-entry toc-h3"><a href="#summary-of-initialization-parameters">Summary of Initialization Parameters</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul><p>This post assumes that you know enough about neural networks to follow through some math involving the backpropagation equations. For the most part, I only do basic algebraic manipulations, with a small dusting of basic statistics thrown in. If you know nothing about neural nets and have an hour or so to spare, the excellent <a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a> is a good place to learn the basics, and getting as far as Chapter 2 should teach you enough to follow the math here. I try here to flesh out some of the math Glorot and Bengio skipped in their paper about initializing weights in deep neural networks, to better illuminate the intution behind why their method solves a longstanding problem facing the training of such networks.</p>

<h2 id="vanishing-gradients">
<a id="vanishing-gradients" class="anchor" href="#vanishing-gradients" aria-hidden="true"><span class="octicon octicon-link"></span></a>Vanishing Gradients</h2>
<p>Initially, one of the challenges preventing the efficient training of very deep neural networks was the phenomenon of extreme gradients. If you look at a plot of the sigmoid activation function, a common choice in vanilla neural networks, it is clear that the derivative approaches zero at the extremes of the function; activations close to 1 or close to 0 both produce very small gradients. A neuron is said to be saturated when its activation occupies these extreme regimes. It was observed during the training of very deep networks that the last hidden layer would often quickly saturate to 0, causing the gradients to be close to 0 as well, which led to the backpropagated gradients in each preceding layer to become smaller and smaller still, until the very first hidden layers felt almost no change to their weights at all from the almost-zero gradients. This is clearly disastrous; the earlier hidden layers are supposed to be busy identifying features in the dataset that successive layers can then use to build more complex features at an even high level of abstraction. If the gradients reaching these early layers do not affect their weights, they end up learning nothing from the dataset, with predictable results on model accuracy.</p>

<p><img src="/assets/sigmoid.png" alt="sigmoid"></p>

<p>In this plot of the sigmoid activation function (the blue line), and its derivative (red line), you can clearly see the regions of saturation (light red background), where forcing the function to go to zero also forces its derivative to go to zero, causing the vanishing gradients issue as you move back through the layers.</p>

<h2 id="glorot-and-bengio">
<a id="glorot-and-bengio" class="anchor" href="#glorot-and-bengio" aria-hidden="true"><span class="octicon octicon-link"></span></a>Glorot and Bengio</h2>
<p>Xavier Glorot and Yoshua Bengio examined the theoretical effects of weight initialization on the vanishing gradients problem in their 2010 paper<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>. The first part of their paper compares activation functions, explaining how certain peculiarities of the commonly-used sigmoid function make it highly susceptible to the problem of saturation, and showing that the hyperbolic tangent and softsign <script type="math/tex">\left( \frac{x}{1 + |x|} \right)</script> activations perform better in this respect.</p>

<p>The second part of their paper considers the problem of initializing weights in a fully connected network, providing theoretical justification for sampling the initial weights from the uniform distribution of a certain variance. The motivating intuition for this is in two parts; for the forward pass, ensuring that the variance of the activations is approximately the same across all the layers of the network allows for information from each training instance to pass through the network smoothly. Similarly, considering the backward pass, relatively similar variances of the gradients allows information to flow smoothly backwards. This ensures that the error data reaches all the layers, so that they can compensate effectively, which is the whole point of training.</p>

<h2 id="notation-and-assumptions">
<a id="notation-and-assumptions" class="anchor" href="#notation-and-assumptions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Notation and Assumptions</h2>
<p>In order to formalize these notions, first we must get some notation out of the way. We have the following definitions:</p>

<ul>
  <li>
<script type="math/tex">a^L</script> is the activation vector for layer <script type="math/tex">L</script>, with dimensions <script type="math/tex">n_L \times 1</script>, where <script type="math/tex">n_L</script> is the number of units in layer <script type="math/tex">L</script>.</li>
  <li>
<script type="math/tex">W^L</script> is the matrix of weights for layer <script type="math/tex">L</script>, with dimensions <script type="math/tex">n_L \times n_{L-1}</script>. Each element <script type="math/tex">W^L_{jk}</script> represents the weight of the connection from neuron <script type="math/tex">k</script> of the current layer to neuron <script type="math/tex">j</script> of the previous one.</li>
  <li>
<script type="math/tex">b^L</script> is the bias vector for layer <script type="math/tex">L</script>, with the same dimensions as <script type="math/tex">a^L</script>.</li>
  <li>
<script type="math/tex">z^L</script> is the weighted input to the activation function of layer <script type="math/tex">L</script>. This means that <script type="math/tex">z^L = W^L \times a^{L-1} + b^L</script>.</li>
  <li>
<script type="math/tex">C</script> is the cost function we’re trying to optimize. Glorot and Bengio use the conditional log likelihood <script type="math/tex">-\log P(y \vert x)</script>, although the details of this won’t matter much.</li>
  <li>
<script type="math/tex">\sigma</script> is the activation function, so that <script type="math/tex">a^L = \sigma (z^L)</script>, where the function is applied to each element of the vector.</li>
  <li>
<script type="math/tex">n_L</script> is the number of units in layer <script type="math/tex">L</script>.</li>
  <li>
<script type="math/tex">x</script> is the input vector to the network.</li>
  <li>
<script type="math/tex">\delta^L = \frac{\delta C}{\delta z^L}</script> is the gradient of the cost function w.r.t. the weighted inputs of layer <script type="math/tex">L</script>, also called the error.</li>
</ul>

<p>The following analysis holds for a fully connected neural network with <script type="math/tex">d</script> layers, with a symmetric activation function with unit derivative at zero. The biases are initialized to zero, and the activation function is approximated by the identity <script type="math/tex">f(x) = x</script> for the initialization period.</p>

<p>We assume that the weights, activations, weighted inputs, raw inputs to the network, and the gradients all come from independent distributions whose parameters depend only on the layer under consideration. Under this assumption, the common scalar variance of the weights of layer <script type="math/tex">L</script> is represented by <script type="math/tex">\text{Var}\left[W^L\right]</script>, with similar representations for the other variables (activations, gradients, etc.)</p>

<!-- Insert image of a fully connected 3 layer net illustrating the notation -->

<h2 id="forward-pass">
<a id="forward-pass" class="anchor" href="#forward-pass" aria-hidden="true"><span class="octicon octicon-link"></span></a>Forward pass</h2>

<p>For the forward pass, we want the layers to keep the input and output variances of the activations equal, so that the activations don’t get amplified or vanish upon successive passes through the layers. Consider <script type="math/tex">z^L_j</script>, the weighted input of unit <script type="math/tex">j</script> in layer <script type="math/tex">L</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
 \text{Var}\left[z^L\right] &= \text{Var}\left[z^L_j\right] \\
 &= \text{Var}\left[ \sum_{k=0}^{n_{L-1}} W^L_{jk} a^{L-1}_k + b^L_j  \right] \\
 &= \text{Var}\left[ \sum_{k=0}^{n_{L-1}} W^L_{jk} a^{L-1}_k \right] \\
 &= \sum_{k=0}^{n_{L-1}} \text{Var}\left[ W^L_{jk} a^{L-1}_k \right] \\
 &= \sum_{k=0}^{n_{L-1}} \text{Var}\left[ W^L_{jk}\right] \text{Var}\left[a^{L-1}_k \right] \\
 &= \sum_{k=0}^{n_{L-1}} \text{Var}\left[ W^L \right] \text{Var}\left[a^{L-1} \right] \\
 &= n_{L-1} \text{Var}\left[ W^L \right] \text{Var}\left[a^{L-1} \right] \\
\end{align*} %]]></script>

<p>In the above simplification, we use the fact that the variance of the sum of two independently random variables is the sum of their variances, under the assumption that the weighted activations would be independent of each other. Later, the variance of the product was expanded out to the product of the variances under the assumption that the weights of the current layer would be independent of the activations of the previous layer. The full expression of the variance of the product of two independent random variables also includes terms containing their means. However, we assume that both activations and weights come from distributions with zero mean, reducing the expression to the product of the variances.</p>

<p>Since the activation function is symmetric, it has value 0 for an input of 0. Furthermore, given that the derivative at 0 is 1, we can approximate the activation function <script type="math/tex">\sigma</script> as the identity during initialization, where the biases are zero and the expected value of the weighted input is zero as well. Under this assumption, <script type="math/tex">a^{L-1} \approx z^{L-1}</script>, which reduces the previous expression to the form of a recurrence:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
 \text{Var}\left[z^L\right] &= n_{L-1} \text{Var}\left[ W^L \right] \text{Var}\left[a^{L-1} \right] \\
 &= n_{L-1} \text{Var}\left[ W^L \right] \text{Var}\left[z^{L-1} \right] \\
 &= n_{L-1} \text{Var}\left[ W^L \right] n_{L-2} \text{Var}\left[ W^{L-1} \right] \text{Var}\left[z^{L-2} \right] \\
 &= \text{Var}\left[ x \right] \prod_{m=0}^{L-1} n_m \text{Var}\left[ W^{m+1} \right]
\end{align*} %]]></script>

<p>Thus, if we want the variances of all the weighted inputs to be the same, the product term must evaluate to <script type="math/tex">1</script>, the easiest way to ensure which is to set <script type="math/tex">\text{Var}\left[ W^{m+1} \right] = \frac{1}{n_m}</script>. Written alternatively, for every layer <script type="math/tex">L</script>, where <script type="math/tex">n_\text{in}</script> is the number of units to the layer (layer fan-in), we want</p>

<script type="math/tex; mode=display">\text{Var}\left[ W^{L} \right] = \frac{1}{n_\text{in}}</script>

<h2 id="backwards-pass">
<a id="backwards-pass" class="anchor" href="#backwards-pass" aria-hidden="true"><span class="octicon octicon-link"></span></a>Backwards pass</h2>
<p>For the backward pass, we want the variances of the gradients to be the same across the layers, so that the gradients don’t vanish or explode prematurely. We use the backpropagation equation as our starting point:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
 \text{Var}\left[\delta^L\right] &= \text{Var}\left[\delta^L_j\right] \\
 &= \text{Var}\left[ \left(\sum_{k=0}^{n_{L+1}} W^{L+1}_{kj} \delta^{L+1}_k \right) \sigma'(z^L_j) \right] \\
 &= \text{Var}\left[ \sum_{k=0}^{n_{L+1}} W^{L+1}_{kj} \delta^{L+1}_k \right] \\
 &= \sum_{k=0}^{n_{L+1}} \text{Var}\left[ W^{L+1}_{kj} \delta^{L+1}_k \right] \\
 &= \sum_{k=0}^{n_{L+1}} \text{Var}\left[ W^{L+1}_{kj} \right] \text{Var}\left[ \delta^{L+1}_k \right] \\
 &= \sum_{k=0}^{n_{L+1}} \text{Var}\left[ W^{L+1} \right] \text{Var}\left[ \delta^{L+1} \right] \\
 &= n_{L+1} \text{Var}\left[ W^{L+1} \right] \text{Var}\left[ \delta^{L+1} \right] \\
 &= n_{L+1} \text{Var}\left[ W^{L+1} \right] n_{L+2} \text{Var}\left[ W^{L+2} \right] \text{Var}\left[ \delta^{L+2} \right] \\
 &= \text{Var}\left[ \delta^d \right]\prod_{m=L+1}^{d-1} n_{m} \text{Var}\left[ W^{m} \right] \\
\end{align*} %]]></script>

<p>Similarly as in the forwards pass, we assume that the gradients are independent of the weights at initilization, and use the variance identities as explained before. Additionally, we make use of the fact that the weighted input <script type="math/tex">z^L</script> has zero mean during the initialization phase in approximating the derivative of the activation function <script type="math/tex">\sigma'(z^L_j)</script> as <script type="math/tex">1</script>. In order to ensure uniform variances in the backwards pass, we obtain the constraint that <script type="math/tex">\text{Var}\left[W^m\right] = \frac{1}{n_m}</script>, which can be written in the following form for every layer <script type="math/tex">L</script> and layer fan-out <script type="math/tex">n_\text{out}</script>:</p>

<script type="math/tex; mode=display">\text{Var}\left[ W^{L} \right] = \frac{1}{n_\text{out}}</script>

<h2 id="conclusion">
<a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2>

<p>In the general case, the fan-in and fan-out of a layer may not be equal, and so as a sort of compromise, Glorot and Bengio suggest using the average of the fan-in and fan-out, proposing that</p>

<script type="math/tex; mode=display">\text{Var}\left[ W^{L} \right] = \frac{2}{n_\text{out} + n_\text{in}}</script>

<p>If sampling from a uniform distribution, this translates to sampling the interval <script type="math/tex">[-a,a]</script>, where <script type="math/tex">a = \sqrt{\frac{6}{n_\text{out} + n_\text{in}}}</script>. The weird-looking <script type="math/tex">\sqrt{6}</script> factor comes from the fact that the variance of a uniform distribution over the interval <script type="math/tex">[-a,a]</script> is <script type="math/tex">a^2/3</script>. Alternatively, the weights can be sampled from a normal distribution with zero mean and variance same as the above expression.</p>

<p>Before this paper, the accepted standard initialization technique was to sample the weights from the uniform distribution over the interval <script type="math/tex">\left[-\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}} \right]</script>, which led to the following variance over the weights: <script type="math/tex">\text{Var}\left[ W^L \right] = \frac{1}{3n^L}</script>. Plugging this into the equations we used for the backward pass, it is clear that the gradients decrease as we go backwards through the layers, reducing by about <script type="math/tex">1/3^\text{rd}</script> at each layer, an effect that is borne out experimentally as well. The paper found that the new initialization method suggested ensured that the gradients remained relatively constant across the layers, and this method is now standard for most Deep Learning applications.</p>

<p>It is interesting that the paper makes the assumption of a symmetric activation function with unit derivative at zero, neither of which conditions is satisfied by the logistic activation function. Indeed, the experimental results in the paper (showing unchanging gradients across layers with the new initialization method) are shown with the <script type="math/tex">tanh</script> activation function, which satisfies both assumptions.</p>

<p>For activation functions like ReLU, He et al. work out the required adjustments in their paper<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>. At a high level, since the ReLU function basically zeroes out an entire half of the domain, it should suffice to compensate by doubling the variance of the weights, a heuristic that matches the result of He’s more nuanced analysis, which suggests that <script type="math/tex">\text{Var}\left[ W^{L} \right] = \frac{4}{n_\text{out} + n_\text{in}}</script> works well.</p>

<h3 id="logistic-activation">
<a id="logistic-activation" class="anchor" href="#logistic-activation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Logistic Activation</h3>
<p>In the forward pass derivation, we approximate the activation function as approximately equal to the identity in the initialization phase we are interested in. For the logistic activation function, the equivalent approximation works out to be <script type="math/tex">\frac{x}{4} + \frac{1}{2}</script> (since the derivative at zero is <script type="math/tex">\frac{1}{4}</script> and the value of the function at zero is <script type="math/tex">\frac{1}{2}</script>), using a truncated Taylor series expansion around 0. Plugging this in,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
 \text{Var}\left[z^L\right] &= n_{L-1} \text{Var}\left[ W^L \right] \text{Var}\left[a^{L-1} \right] \\
 &= n_{L-1} \text{Var}\left[ W^L \right] \text{Var}\left[\frac{z^{L-1}}{4} + \frac{1}{2} \right] \\
 &= n_{L-1} \text{Var}\left[ W^L \right] \text{Var}\left[\frac{z^{L-1}}{4} \right] \\
 &= \frac{n_{L-1}}{16} \text{Var}\left[ W^L \right] \text{Var}\left[z^{L-1} \right] \\
\end{align*} %]]></script>

<p>The remaining steps are identical, except for the factor <script type="math/tex">1/16</script> in front.</p>

<p>Similarly in the backward pass, where we ignored the derivative of the activation function under the assumption that it was zero, plugging in the correct value of <script type="math/tex">1/4</script> results in a factor of <script type="math/tex">1/16</script> in this case as well.</p>

<p>Together, since this factor of <script type="math/tex">1/16</script> appears identically in both passes, it follows through into the fan-in and fan-out numbers, producing the constraint that</p>

<script type="math/tex; mode=display">\text{Var}\left[ W^{L} \right] = \frac{32}{n_\text{out} + n_\text{in}}</script>

<h3 id="summary-of-initialization-parameters">
<a id="summary-of-initialization-parameters" class="anchor" href="#summary-of-initialization-parameters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary of Initialization Parameters</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Activation Function</th>
      <th style="text-align: center">Uniform Distribution <script type="math/tex">[-a,a]</script>
</th>
      <th style="text-align: center">Normal distribution</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Logistic</td>
      <td style="text-align: center"><script type="math/tex">a = 4\sqrt{\frac{6}{n_\text{out} + n_\text{in}}}</script></td>
      <td style="text-align: center"><script type="math/tex">\sigma =  4\sqrt{\frac{2}{n_\text{out} + n_\text{in}}}</script></td>
    </tr>
    <tr>
      <td style="text-align: center">Hyperbolic Tangent</td>
      <td style="text-align: center"><script type="math/tex">a = \sqrt{\frac{6}{n_\text{out} + n_\text{in}}}</script></td>
      <td style="text-align: center"><script type="math/tex">\sigma =  \sqrt{\frac{2}{n_\text{out} + n_\text{in}}}</script></td>
    </tr>
    <tr>
      <td style="text-align: center">ReLU</td>
      <td style="text-align: center"><script type="math/tex">a = \sqrt{\frac{12}{n_\text{out} + n_\text{in}}}</script></td>
      <td style="text-align: center"><script type="math/tex">\sigma =  \sqrt{\frac{12}{n_\text{out} + n_\text{in}}}</script></td>
    </tr>
  </tbody>
</table>

<h2 id="references">
<a id="references" class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a> <a href="#fnref:1" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:2">
      <p><a href="https://arxiv.org/pdf/1502.01852.pdf">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a> <a href="#fnref:2" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2018/08/09/lattice-basis-reduction-part-2/">
            Lattice Basis Reduction Part 2
            <small>09 Aug 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2018/08/09/lattice-basis-reduction-part-1/">
            Lattice Basis Reduction Part 1
            <small>09 Aug 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2018/06/26/quantum-crypto-part-4/">
            Quantum Crypto Part 4
            <small>26 Jun 2018</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
