<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Manas George</title>
 <link href="www.mnsgrg.com/atom.xml" rel="self"/>
 <link href="www.mnsgrg.com/"/>
 <updated>2018-08-09T22:35:36-07:00</updated>
 <id>www.mnsgrg.com</id>
 <author>
   <name></name>
   <email></email>
 </author>

 
 <entry>
   <title>Lattice Basis Reduction Part 1</title>
   <link href="www.mnsgrg.com/2018/08/09/lattice-basis-reduction-part-1/"/>
   <updated>2018-08-09T00:00:00-07:00</updated>
   <id>www.mnsgrg.com/2018/08/09/lattice-basis-reduction-part-1</id>
   <content type="html">&lt;p&gt;The first part of this series covers what a lattice is, what a basis for a given lattice is, what it means for that basis to be “reduced”, and the LLL algorithm, which gives us a powerful tool to take an arbitrary lattice basis and try to make it as small as possible, which turns out to be useful for mysterious reasons.&lt;/p&gt;

&lt;h2 id=&quot;lattices&quot;&gt;Lattices&lt;/h2&gt;

&lt;p&gt;A lattice (not to be confused with a lettuce) is a subspace of the vector space &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^n&lt;/script&gt;; it is the closure of a set of vectors in that space under addition with integer coefficients. If that mouthful sounds too confusing, think of it as a set of evenly spaced grid points, like in the picture below. Given a basis of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; linearly independent vectors &lt;script type=&quot;math/tex&quot;&gt;B = \{ b_1, b_2, \dots, b_n\}&lt;/script&gt;, the lattice &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; that they span is defined as the set &lt;script type=&quot;math/tex&quot;&gt;\left \{ \sum\limits_{i=1}^n z_i b_i \vert z_i \in \mathbb{Z} \land b_i \in B \right \}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The diagram below depicts an example of a 2-dimensional lattice with basis vectors &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
b_1 = \begin{bmatrix} 1.1 &amp; 0.3 \end{bmatrix} %]]&gt;&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
b_2 = \begin{bmatrix} 1.3 &amp; 0.5 \end{bmatrix} %]]&gt;&lt;/script&gt;. Note that the elements that make up the basis vectors themselves need not be integers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/lattice.png&quot; alt=&quot;lattice&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each of the other lattice points in blue is generated by some linear combination of these two basis vectors. The basis vectors for a given lattice are not unique. In particular, the two vectors in green form an alternate basis for the same lattice. You will notice that the green vectors are shorter and more orthogonal than the red vectors. It turns out having shorter, more orthogonal basis vectors given an arbitrary lattice makes it much easier to work with them, and indirectly lead to solutions to interesting problems. If you were some kind of grad student doing math, then, you might wonder whether there exists a well-defined procedure that takes an arbitrary lattice basis and produce a shorter basis for the same lattice.&lt;/p&gt;

&lt;p&gt;Why would anybody wonder about such a silly thing? As a concrete motivating problem, suppose we would like to find out whether a certain number &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; is algebraic; that is, if it can be expressed as one of the roots to a polynomial of degree &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;. We will use the lattice reducing method discussed below (the famous LLL algorithm) to find an equation such that one of its roots is &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;. The general plan of attack is to set up a lattice such that short vectors in that basis correspond to an equation that contains &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; as a root.&lt;/p&gt;

&lt;h2 id=&quot;the-2-dimensional-case&quot;&gt;The 2-Dimensional Case&lt;/h2&gt;

&lt;p&gt;It is easiest to start with the problem for two dimensions; we can readily visualize the vectors involved and rely on our intuition to guide us. Starting with the vectors &lt;script type=&quot;math/tex&quot;&gt;b_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b_2&lt;/script&gt;, we wish to reduce them as much as possible. That is to say, we’d like to make the two vectors as short as possible without changing their span. How do we know when we are done? As one condition, let us require that &lt;script type=&quot;math/tex&quot;&gt;\lVert b_1 \rVert \le \lVert b_2 \rVert&lt;/script&gt;. This imposes an ordering on the reduced basis vectors, which is nice to have, because it means we can be sure that given a solution, we aren’t accidentally ignoring other possible solutions. How do we know that we’ve reached the shortest possible vectors? Consider the operations we are allowed to perform on the basis without changing the span of the basis:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We can exchange two basis vectors; clearly this doesn’t change the span of the basis&lt;/li&gt;
  &lt;li&gt;We can replace a basis vector by an integer linear combination of basis vectors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first operation lets us ensure that the vectors are in increasing order of their norms. The second lets us reduce the norms of the vectors by subtracting appropriate multiples of the other basis vector. Taking inspiration from Euclid’s algorithm for calculating the GCD of two numbers, we start by subtracting as many multiples of the smaller vector as possible from the larger one. Without loss of generality, assume &lt;script type=&quot;math/tex&quot;&gt;b_1&lt;/script&gt; is the smaller vector (we can always exchange vectors to make this the case).&lt;/p&gt;

&lt;p&gt;It turns out that taking the projection of the larger vector on to the smaller gives us the largest scalar multiple of the smaller vector we can subtract from the larger without increasing the norm of the resulting vector &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Our first step, then is to update &lt;script type=&quot;math/tex&quot;&gt;b_2&lt;/script&gt; as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{21} &amp;=   \frac{\langle b_1, b_2 \rangle}{\langle b_1, b_1 \rangle}  \\
b_2 &amp;:= b_2 -  \text{nint} \left( \mu_{21} \right) b_1
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that we do not use the projection &lt;script type=&quot;math/tex&quot;&gt;\mu_{21}&lt;/script&gt; directly; we must round to the nearest integer so that the resulting vector remains within the lattice. Once we do this, we cannot reduce &lt;script type=&quot;math/tex&quot;&gt;b_2&lt;/script&gt; any further; adding or subtracting any multiple of &lt;script type=&quot;math/tex&quot;&gt;b_1&lt;/script&gt; from &lt;script type=&quot;math/tex&quot;&gt;b_2&lt;/script&gt; now results in a larger vector. If the new &lt;script type=&quot;math/tex&quot;&gt;b_2&lt;/script&gt; is smaller than &lt;script type=&quot;math/tex&quot;&gt;b_1&lt;/script&gt;, we don’t quite yet have the smallest possible basis. We exchange the two vectors and repeat the reduction process until it is no longer possible to reduce &lt;script type=&quot;math/tex&quot;&gt;b_2&lt;/script&gt;. This corresponds to the case where &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\left\vert \frac{\langle b_1, b_2 \rangle}{\langle b_1, b_1 \rangle} \right\vert &lt; \frac{1}{2} %]]&gt;&lt;/script&gt;, which means that the nearest integer is 0, implying that subtracting a multiple of &lt;script type=&quot;math/tex&quot;&gt;b_1&lt;/script&gt; from &lt;script type=&quot;math/tex&quot;&gt;b_2&lt;/script&gt; now can only make it bigger. At this point, we stop, ending up with vectors &lt;script type=&quot;math/tex&quot;&gt;b^*_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b^*_2&lt;/script&gt; that satisfy the conditions:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left\Vert b^*_1 \right\Vert &amp;\le \left\Vert b^*_2 \right\Vert \\
\left\vert \frac{\langle b^*_1, b^*_2 \rangle}{\langle b^*_1, b^*_1 \rangle} \right\vert = \mu_{21} &amp;\le \frac{1}{2}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We take these conditions to be the definition of a reduced basis, since we cannot further reduce vectors satisfying these conditions using span-preserving operations. The algorithm above (reduce, exchange, repeat) is due to Gauss, although it is also sometimes called Lagrange’s algorithm.&lt;/p&gt;

&lt;h2 id=&quot;generalizing-to-higher-dimensions&quot;&gt;Generalizing to higher dimensions&lt;/h2&gt;
&lt;h3 id=&quot;gram-schmidt-orthogonalization-review&quot;&gt;Gram Schmidt Orthogonalization Review&lt;/h3&gt;
&lt;p&gt;For a set of vectors &lt;script type=&quot;math/tex&quot;&gt;B = \{ b_1, b_2, \dots, b_n\}&lt;/script&gt;, we define the Gram-Schmidt orthogonalization coefficient &lt;script type=&quot;math/tex&quot;&gt;\mu_{ij} =  \frac{\langle b_i, b^*_j \rangle}{\langle b^*_j, b^*_j \rangle}&lt;/script&gt;. Here, &lt;script type=&quot;math/tex&quot;&gt;b^*_i&lt;/script&gt; is the result of performing the Gram-Schmidt orthogonalization process on the vector &lt;script type=&quot;math/tex&quot;&gt;b_i&lt;/script&gt;, in the vector space that &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; spans. We have &lt;script type=&quot;math/tex&quot;&gt;b^*_1 = b_1&lt;/script&gt; by definition. For the remaining vectors, &lt;script type=&quot;math/tex&quot;&gt;b^*_i&lt;/script&gt; is the portion of &lt;script type=&quot;math/tex&quot;&gt;b_i&lt;/script&gt; that captures some element of the space spanned by $B$ that the vectors before it &lt;script type=&quot;math/tex&quot;&gt;b_1, \cdots, b_{i-1}&lt;/script&gt; do not capture. Translating this into concrete formulae;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
b^*_i &amp;= b_i - \text{projection of } b_i \text{ on to } \text{span}(b_1, \cdots, b_{i-1}) \\
&amp;= b_i - \text{projection of } b_i \text{ on to } \text{span}(b^*_1, \cdots, b^*_{i-1}) \\
&amp;= b_i - \sum_{j=1}^{i-1} \mu_{ij} b^*_j 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;reduction-step-for-lll&quot;&gt;Reduction step for LLL&lt;/h3&gt;
&lt;p&gt;The Gram Schmidt vectors are usually normalized, so that &lt;script type=&quot;math/tex&quot;&gt;\left\Vert b^*_i \right\Vert = 1&lt;/script&gt;. We leave them un-normalized here; the norms of the vectors are arbitrary. The Gram Schmidt basis gives us a kind of ideal baseline to measure against while reducing the lattice basis; it is the shortest, most orthogonal basis we could have found, unconstrained by the tyranny of integer multiples. It leads somewhat naturally to the definition of an LLL-reduced basis. In the reduction step, we can proceed as in the 2-dimensional case, generalized in the manner of the Gram Schmidt orthogonalization process. We reduce &lt;script type=&quot;math/tex&quot;&gt;b_i&lt;/script&gt; by replacing it as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_i :=  b_i - \sum_{j=1}^{i-1} \text{nint} \left(\mu_{ij} \right) b^*_j&lt;/script&gt;

&lt;p&gt;Where &lt;script type=&quot;math/tex&quot;&gt;\text{nint}&lt;/script&gt; is just the nearest integer function. Now that we have a way to reduce the lengths of our basis vectors, we can define the first condition required for an LLL-reduced basis in higher dimensions, a straightforward extension of the condition for 2 dimensions. We require that $\lvert \mu_{ij} \le \frac{1}{2} \rvert $ for $ 1 \le j &amp;lt; i \le n $, using the same reasoning as we did for the 2 dimensional case that once this condition is met, further norm reduction within the lattice is not possible.&lt;/p&gt;

&lt;h3 id=&quot;imposing-an-ordering&quot;&gt;Imposing an ordering&lt;/h3&gt;
&lt;p&gt;In the 2 dimensional case, we had an obvious way of ordering the basis vectors, essentially comparing vector magnitudes within a plane. In the higher dimensional case, consider the problem of comparing the reduced vectors $b_i$ and $b_{i+1}$. Directly comparing their norms gets us nowhere, since in the ideal case all the reduced vectors have the same (unit) norm and we would be comparing vectors with different dimensions&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. Instead, we must compare projections of the vectors onto some subspace in order to get a more varied measure of length that we can then use to compare the vectors. For $b_i$, we use the norm $ \lVert {b_i^*} \rVert $ as the required measure; the length of $b_i$ when it is stripped of the components that lie along the vectors &lt;script type=&quot;math/tex&quot;&gt;b_1, \cdots, b_{i-1}&lt;/script&gt; that come before it. Similarly, we strip $b_{i+1}$ of its components along the same vectors &lt;script type=&quot;math/tex&quot;&gt;b_1, \cdots, b_{i-1}&lt;/script&gt;, taking the length&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp; \lVert b_{i+1} - \sum_{j=1}^{i-1} \mu_{(i+1)j} b^*_j \rVert \\
&amp;= \lVert b_{i+1} - \sum_{j=1}^{i} \mu_{(i+1)j} b^*_j + \mu_{(i+1)i} b^*_i \rVert \\
&amp;= \lVert b^*_{i+1} + \mu_{(i+1)i} b^*_i \rVert \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The goal here is to normalize both vectors by restricting them to the same subspace; by removing components of the same vectors, we effectively project them into the subspace $S_i$ that is the orthogonal complement of the span of the first $i-1$ vectors, $\text{ span}(b_1, \cdots, b_{i-1})$. This allows us to ensure an ordering using a meaningful measure of length, since the vectors are restricted to the same subspace and therefore have equal rank.&lt;/p&gt;

&lt;p&gt;This gives us our ordering condition; we want the first vector to be at most the second vector by the measures we have devised:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lVert {b_i^*} \rVert \le \lVert b^*_{i+1} + \mu_{(i+1)i} b^*_i \rVert&lt;/script&gt;

&lt;p&gt;The constraint here turns out to be too loose to ensure convergence in polynomial time, so in practice we want the difference between the measures to be larger. It suffices to require that the first vector is at most a small multiple $\delta$ of the second, where it turns out &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; that any $ 1 &amp;lt; \delta &amp;lt; 4 $ will do. For simplicity, we set $\delta = \frac{4}{3}$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lVert {b_i^*} \rVert \le \delta \lVert b^*_{i+1} + \mu_{(i+1)i} b^*_i \rVert&lt;/script&gt;

&lt;h3 id=&quot;the-complete-algorithm&quot;&gt;The complete algorithm&lt;/h3&gt;
&lt;p&gt;Given the two conditions for a reduced basis, the final algorithm pops out almost trivially; starting from the given basis, apply the reduction step. After the reduction step has been performed, check consecutive reduced basis vectors to see if they satisfy the ordering constraint. If they do not, swap the two vectors so that they do. If at the end of this, the reduction constraint has been satisfied, we are done. Otherwise, we repeat the whole thing (reduction, swap if necessary, repeat) until both constraints are satisfied. The original paper &lt;sup id=&quot;fnref:3:1&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; proves that this procedure is guaranteed to terminate in polynomial time, resulting in a reduced basis. It also proves additional constraints on the basis, showing that it is “short” in the sense that the shortest vector in the basis is at most exponentially larger than the shortest vector in the entire lattice. The exponential bound may not seem all that impressive, but it tends to work fairly well in practice, and we will see later that it is good enough to solve other problems of interest.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Consider the inner product of a potentially reduced vector; &lt;script type=&quot;math/tex&quot;&gt;\langle b_2 - c b_1, b_2 - c b_1 \rangle = \langle b_2, b_2 \rangle - 2c \langle b_1, b_2 \rangle + c^2 \langle b_1, b_1 \rangle&lt;/script&gt;. In order to minimize this expression, take the derivative of the inner product with respect to &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; and set it to zero, giving &lt;script type=&quot;math/tex&quot;&gt;-2 \langle b_1, b_2 \rangle + 2c \langle b_1, b_1 \rangle = 0 \implies c = \frac{\langle b_1, b_2 \rangle}{\langle b_1, b_1 \rangle}&lt;/script&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Technically speaking, we would be comparing vectors that are projections into subspaces with different ranks; $ b_i^* $ is the projection of $b_i$ into the orthogonal complement of $\text{span}(b_1, \cdots, b_{i-1})$ which is of rank $n-i+1$, and $b_{i+1}^* $ is the the projection of $b_{i+1}$ into the orthogonal complement of $\text{span}(b_1, \cdots, b_{i})$ which is of rank $n-i$. This would effectively be the same as comparing vectors of different dimensions, in the sense that the second vector would be predictably biased by having an extra coordinate’s worth of freedom to vary in, affecting the norm. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://link.springer.com/article/10.1007%2FBF01457454&quot;&gt;Lenstra, Arjen Klaas, Hendrik Willem Lenstra, and László Lovász. “Factoring polynomials with rational coefficients.” Mathematische Annalen 261.4 (1982): 515-534.&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:3:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Quantum Crypto Part 4</title>
   <link href="www.mnsgrg.com/2018/06/26/quantum-crypto-part-4/"/>
   <updated>2018-06-26T00:00:00-07:00</updated>
   <id>www.mnsgrg.com/2018/06/26/quantum-crypto-part-4</id>
   <content type="html">&lt;p&gt;This is the last in the Quantum Cryptography series of posts. This post
covers the limitations of current methods and contemplates
possible avenues of evolution for the technology.&lt;/p&gt;

&lt;h1 id=&quot;limitations&quot;&gt;Limitations&lt;/h1&gt;

&lt;h2 id=&quot;distance&quot;&gt;Distance&lt;/h2&gt;

&lt;p&gt;Most QKD systems today rely on optical transmission of photons, either
via telecommunications fiber or free space. Both media pose technical
challenges in terms of attenuation and preserving polarization that must
be overcome if global QKD is to become a reality. The primary form of
noise in photonic transmissions is loss over distances &lt;a href=&quot;#Sangouard2011&quot;&gt;[1]&lt;/a&gt;.
Currently, the best systems known span distances of between 100 and
140km (missing reference). Classical communications work around the
problem of attenuation by using repeaters, something that is difficult
to emulate in the quantum regime due to the no-cloning theorem. A
working quantum repeater would require matter quantum memory
(missing reference), and might even be more difficult than the
task of developing a universal quantum computer, due to the constraints
involved (in particular, the quantum memories in question must satisfy
DiVincenzo’s five criteria for universal quantum computers, as well as
his additional (harder) criterion. &lt;a href=&quot;#DiVincenzo2000&quot;&gt;[2]&lt;/a&gt;). However, the
forefront of research today suggests alternate formulations of quantum
repeaters that do away with the requirement for quantum memories and may
provide a feasible way to extend the range of QKD &lt;a href=&quot;#Azuma2015&quot;&gt;[3]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;speed&quot;&gt;Speed&lt;/h2&gt;

&lt;p&gt;Current QKD systems are limited in key transfer rate by a multitude of
factors; the sifting process, transmission errors, detector
inefficiencies, attenuation, additionally key bits sacrificed to privacy
amplification, as well as other influences all combine to reduce the
number of secure viable key bits transmitted in a QKD link. The
transmission rate is intricately tied to the range of the system;
increased range decreases the probability that a qubit reaches the
destination receiver, and attenuation along the way may mean that
detection probability is lowered even if the destination is reached. The
rate also depends on the details of the system used; certain approaches
offer higher rates than others. The experimental SECOQC project
&lt;a href=&quot;#Peev2009&quot;&gt;[4]&lt;/a&gt; recommended a baseline key transmission rate of 1kbit/s over
a 25km link, which was exceeded handily by some systems used in the
project, with even higher rates at shorter distances, as high as 50
kbits/s over an 80m free space link &lt;a href=&quot;#Peev2009&quot;&gt;[4]&lt;/a&gt;. In fact, rates as high
as 1.02 Mbit/s have been seen at distances of 20km using fiber links
&lt;a href=&quot;#Dixon2008&quot;&gt;[5]&lt;/a&gt;. In comparison to traditional communications
infrastructure, such rates are almost laughably small, which means that
QKD is likely to be used only for the highest security applications in
the near future. However, advances in high-speed single photon detectors
&lt;a href=&quot;#Yuan2007&quot;&gt;[6]&lt;/a&gt; promise, among others, promise far higher rates in years to
come.&lt;/p&gt;

&lt;h2 id=&quot;cost&quot;&gt;Cost&lt;/h2&gt;

&lt;p&gt;A significant obstacle to the widespread adoption of QKD systems is the
high cost of setting up and maintaining the equipment required for their
use. Here, the use of telecommunications fiber, as well as the reliance
on classical communications technologies for certain parts of the
protocols mitigate some of the investment, as existing infrastructure
can be reused. On the other hand, specialized equipment for the
generation of entangled pairs, for instance, or the modulation of weak
light pulses, require capital investments that are many orders of
magnitude higher than traditional cryptographic systems. The emergence
of a number of companies in the QKD space (ID Quantique, MagiQ
Technologies, Swiss Quantum, Battelle, etc.) are a promising indicator
that demand for such systems is healthy in environments where security
is of paramount importance, and the prevalent mood is that mass
production and economies of scale, coupled with technological advances
in the manufacture of the components required (detectors, especially),
will eventually bring the cost of QKD systems down to a manageable
amount. Furthermore, QKD systems are often much cheaper than the
alternative in many high security environments, where the prevalent
method of key distribution is the use of couriers to physically exchange
random key material, with a much lower security guarantee.&lt;/p&gt;

&lt;h1 id=&quot;future&quot;&gt;Future&lt;/h1&gt;

&lt;h2 id=&quot;possible-adoption-trajectories&quot;&gt;Possible adoption trajectories&lt;/h2&gt;

&lt;p&gt;The types of environments best suited for the balance of security and
cost offered by QKD networks are controlled, high-value organizations
that require extremely strong security guarantees over long periods of
time. QKD, combined with one-time pad encryption, offers &lt;em&gt;unconditional
security&lt;/em&gt;, as defined by Diffie and Hellman &lt;a href=&quot;#Diffie1976&quot;&gt;[7]&lt;/a&gt;, which
protects secrets from any form of cryptanalysis, no manner how
computationally or mathematically advanced. Compare this with the
vulnerability of traditional encryption schemes to expanding
computational power over time; leaps in the amount of processing power
possessed by novel systems reduce the complexity of a brute force attack
on, say, 256-bit AES to the point where ciphertexts from today, stored
by an adversary until such time in the future as computational power has
reached the point where it is feasible to perform cryptanalysis on such
ciphertexts, maybe decrypted in the future. Only the one-time pad offers
any guarantee against such attacks, and only QKD has the potential to
distribute keys for use in the one-time pad in a relatively efficient,
scalable manner. Therefore, sensible predictions about the adoption of
QKD must begin with environments such as government organizations,
financial institutions, global military organizations, among others. It
is possible that QKD technology may follow a trajectory not too
different from that of the internet; beginning as the sole province of
research institutions and government-funded projects, and then gradually
expanding to encompass businesses and industry, eventually reaching a
sort of threshold where widespread mass adoption becomes possible, and
the dream of provably secure communication for all becomes true.&lt;/p&gt;

&lt;h2 id=&quot;satellite-based-qkd&quot;&gt;Satellite-based QKD&lt;/h2&gt;

&lt;p&gt;One of the most attractive proposals for a global QKD system leverages
free space transmissions between ground and satellite systems for key
exchange. Development in this area is well underway. A research effort
from 2002 demonstrated secure key exchange over a free space link at
23.4km &lt;a href=&quot;#Kurtsiefer2002&quot;&gt;[8]&lt;/a&gt;, with the attenuation results obtained
indicating that near earth key exchange (at a range of 500-1,000km)
should be possible in the near future. It has been almost 13 years since
this result, and the proposal made by that paper seems achievable at
present. A collaboration between industry partners and the Canadian
Space agency over a mission called QEYSSat (Quantum Encryption and
Science Satellite) is underway, proposing the use of a microsatellite
located in low earth orbit at about 600km carrying an optical receiver
with 40cm aperture as the main optical instrument. The group has
conducted studies of transmission losses in such regimes, successfully
operating a system at upto 60dB losses &lt;a href=&quot;#Meyer-Scott2011&quot;&gt;[9]&lt;/a&gt;, even in the
face of strong turbulence, countered with careful post-processing
&lt;a href=&quot;#Erven2012&quot;&gt;[10]&lt;/a&gt;. Ensuring the robust and reliable operation of such a
service remains a goal for the future, paving the way for optical links
to low earth satellites that may eventually form the basis for a global
system of QKD. Further applications of satellite QKD could be in the
secure distribution of keys for satellite remote access and for secure
inter-satellite links &lt;a href=&quot;#PerdiguesArmengol2008&quot;&gt;[11]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/quantum/satellite.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;#Jennewein2014&quot;&gt;[12]&lt;/a&gt; An artist’s rendering of QKD via a trusted satellite node&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-quantum-internet&quot;&gt;The Quantum Internet &lt;a href=&quot;#Kimble2008&quot;&gt;[13]&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;One of the key results of research into quantum cryptography is the
maturation of technologies that allow for the creation of purely quantum
links between systems, transporting quantum states between
geographically separated sites with high fidelity, maintaining important
quantum properties like entanglement. Carrying out this idea to its
logical conclusion leads to the idea of a quantum internet, whereby a
network of computational nodes linked by quantum channels would be
empowered to perform computational tasks beyond classical physics. For
instance, Preskill’s notion of quantum software &lt;a href=&quot;#Preskill1999&quot;&gt;[14]&lt;/a&gt;, used to
describe difficult-to-create quantum states that perform useful quantum
computations, would find a distribution mechanism in such a quantum
internet. Perhaps most dazzling is the exponential increase in state
space that would be produced by full quantum connectivity between nodes
of such a network; a fully quantum network of $n$ nodes each with $k$
quantum bits would have a state space on the order of $2^{kn}$, whereas
such a network that utilized only classical connections would have a
state space of significantly lower dimension, on the order of $n2^k$.
However, fully realizing such a system would require overcoming rather
large technical difficulties in local quantum processing, quantum
repeaters, and error-corrected quantum teleportation, as well as
advances in quantum memory. Theoretically, the developments required
would signal a shift in focus from concentrating on highly specialized
components (say, systems comprising of single electrons trapped in
crystals) to more complex dynamical quantum systems composed of many
such building blocks.&lt;/p&gt;

&lt;h1 id=&quot;bibliography&quot;&gt;Bibliography&lt;/h1&gt;
&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;





&lt;span id=&quot;Sangouard2011&quot;&gt;N. Sangouard, C. Simon, H. De Riedmatten, and N. Gisin, “Quantum repeaters based on atomic ensembles and linear optics,” &lt;i&gt;Rev. Mod. Phys.&lt;/i&gt;, vol. 83, no. 1, pp. 33–80, 2011.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://arxiv.org/abs/quant-ph/0002077&quot;&gt;






&lt;span id=&quot;DiVincenzo2000&quot;&gt;D. P. DiVincenzo and IBM, “The Physical Implementation of Quantum Computation,” &lt;i&gt;Fortschritte der Phys.&lt;/i&gt;, vol. 48, pp. 771–783, 2000.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://www.nature.com/doifinder/10.1038/ncomms7787&quot;&gt;






&lt;span id=&quot;Azuma2015&quot;&gt;K. Azuma, K. Tamaki, and H.-K. Lo, “All-photonic quantum repeaters,” &lt;i&gt;Nat. Commun.&lt;/i&gt;, vol. 6, p. 6787, 2015.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://iopscience.iop.org/1367-2630/11/7/075001&quot;&gt;






&lt;span id=&quot;Peev2009&quot;&gt;M. Peev, C. Pacher, and R. Alléaume, “The SECOQC quantum key distribution network in Vienna,” &lt;i&gt;New J. Phys.&lt;/i&gt;, vol. 11, p. 075001, 2009.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Dixon2008&quot;&gt;a R. Dixon, Z. L. Yuan, J. F. Dynes, a W. Sharpe, and a J. Shields, “Gigahertz decoy quantum key distribution with 1 Mbit/s secure key rate.,” &lt;i&gt;Opt. Express&lt;/i&gt;, vol. 16, no. 23, pp. 18790–18797, 2008.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Yuan2007&quot;&gt;Z. L. Yuan, B. E. Kardynal, A. W. Sharpe, and A. J. Shields, “High speed single photon detection in the near infrared,” &lt;i&gt;Appl. Phys. Lett.&lt;/i&gt;, vol. 91, no. 4, pp. 41113–41114, 2007.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1055638&quot;&gt;






&lt;span id=&quot;Diffie1976&quot;&gt;W. Diffie and M. Hellman, “New directions in cryptography,” &lt;i&gt;IEEE Trans. Inf. Theory&lt;/i&gt;, vol. 22, no. 6, pp. 644–654, 1976.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Kurtsiefer2002&quot;&gt;C. Kurtsiefer &lt;i&gt;et al.&lt;/i&gt;, “A step towards global key distribution.,” &lt;i&gt;Nature&lt;/i&gt;, vol. 419, no. 6906, p. 450, 2002.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Meyer-Scott2011&quot;&gt;E. Meyer-Scott, Z. Yan, A. MacDonald, J. P. Bourgoin, H. Hübel, and T. Jennewein, “How to implement decoy-state quantum key distribution for a satellite uplink with 50-dB channel loss,” &lt;i&gt;Phys. Rev. A - At. Mol. Opt. Phys.&lt;/i&gt;, vol. 84, no. 6, pp. 1–9, 2011.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Erven2012&quot;&gt;C. Erven &lt;i&gt;et al.&lt;/i&gt;, “Studying free-space transmission statistics and improving free-space quantum key distribution in the turbulent atmosphere,” &lt;i&gt;New J. Phys.&lt;/i&gt;, vol. 14, pp. 1–20, 2012.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;PerdiguesArmengol2008&quot;&gt;J. M. Perdigues Armengol &lt;i&gt;et al.&lt;/i&gt;, “Quantum communications at ESA: Towards a space experiment on the ISS,” &lt;i&gt;Acta Astronaut.&lt;/i&gt;, vol. 63, no. 1-4, pp. 165–178, 2008.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://dx.doi.org/10.1117/12.2041693&quot;&gt;






&lt;span id=&quot;Jennewein2014&quot;&gt;T. Jennewein &lt;i&gt;et al.&lt;/i&gt;, “QEYSSAT: a mission proposal for a quantum receiver in space,” vol. 8997, pp. 89970A–89970A–7, 2014.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Kimble2008&quot;&gt;H. J. Kimble, “The quantum internet.,” &lt;i&gt;Nature&lt;/i&gt;, vol. 453, no. 7198, pp. 1023–1030, 2008.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Preskill1999&quot;&gt;J. Preskill, “Plug-in quantum software,” &lt;i&gt;Nature&lt;/i&gt;, vol. 402, no. 6760, pp. 357–358, 1999.&lt;/span&gt;



&lt;/li&gt;&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Quantum Crypto Part 3</title>
   <link href="www.mnsgrg.com/2018/06/05/quantum-crypto-part-3/"/>
   <updated>2018-06-05T00:00:00-07:00</updated>
   <id>www.mnsgrg.com/2018/06/05/quantum-crypto-part-3</id>
   <content type="html">&lt;p&gt;This is the third in the Quantum Cryptography series of posts. This post
covers some of the experiments undertaken in the field already.&lt;/p&gt;

&lt;p&gt;The practical utility of a functional system of quantum key distribution
(QKD) is exceedingly high. It is perhaps too early to envision a future
where all secure transmissions occur over quantum channels, although
rapid progress is being made in pursuit of such a lofty goal.
Traditional QKD is distance limited, and can only proceed over a single
physical channel (either free space or telecommunications fiber, but not
at the same time in series due to issues with frequency propagation and
modulation &lt;a href=&quot;#Elliott2005&quot;&gt;[1]&lt;/a&gt;). Furthermore, fiber cuts or intensive
eavesdropping result in denial of service, rendering a single link
effectively useless. However, a QKD network bypasses many of these
limitations to a surprising degree, in comparison to standalone QKD
systems. This section outlines some of the results that have been
obtained in recent years pertaining to the establishment and maintenance
of QKD networks.&lt;/p&gt;

&lt;h2 id=&quot;quantum-networks&quot;&gt;Quantum Networks&lt;/h2&gt;

&lt;p&gt;QKD is fundamentally limited by the nature of the links possible between
devices; barring exotic protocols that are the subject of ongoing
research, almost all QKD protocols deal with point to point distribution
of keys, with the goal of establishing a shared secret between a pair of
users. This means that a transition to a networked distribution process,
where more than two users are involved, is relatively difficult, and
must be accomplished through additional protocols layered over the
fundamental choice of QKD protocol. &lt;a href=&quot;#Alleaume2014&quot;&gt;[2]&lt;/a&gt; characterizes
quantum networks in the following ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Optically switched quantum networks: In such networks, some optical
operation (switching, multiplexing, beam-splitting, etc.) can be
used to extend a network to multiple users. This can be done in two
ways; passive switching, whereby a single beam of photons may be
split into multiple, weaker beams, by randomly splitting single
photons &lt;a href=&quot;#Townsend1994&quot;&gt;[3]&lt;/a&gt;, or active switching, where two nodes of the
network can be connected through a single quantum link by
appropriately switching connections at other nodes so as to route
the transmission correctly. Both methods effectively extend the
quantum channel from one node to another with no interruption, and
therefore carries no trust requirement on individual nodes; the same
eavesdropping guarantees that secure single pair quantum
transmissions are true for switched quantum channels, as the result
is effectively a single, uninterrupted quantum channel that extends
across multiple nodes &lt;a href=&quot;#Alleaume2014&quot;&gt;[2]&lt;/a&gt;. Note, however, that this does
not physically extend the range of the network, due to optical
losses which effectively reduce the maximum range of the network.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;“Full” quantum networks: A true quantum network would need to use
quantum repeaters to overcome signal losses over long distances,
removing the requirement that intermediate nodes be trusted, much in
the same manner as optically switched quantum networks. However,
quantum repeaters involve quantum memory and elaborate quantum
operations that are currently outside the scope of physical
realization, although the quantum memory requirement is being
challenged, for instance, by &lt;a href=&quot;#Azuma2015&quot;&gt;[4]&lt;/a&gt;. An alternative would be
to use quantum relays, which are simpler, as they do not require
quantum memory to implement. However, current technological
limitations make quantum relays impractical for arbitrary range
extensions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Trusted Repeater QKD networks: This technique leverages classical
memory to store local keys on every node, allowing for secure
transmission of information between nodes using the local keys
(using a One Time Pad). The local keys are replenished using QKD and
are used for both unconditionally secure encryption and
authentication. For global key transmission, the global key is sent,
hop, by hop, along a series of nodes starting from the source and
ending at the destination node. At each hop, the global key is
encrypted and authenticated using the local key for that link,
guaranteeing security as long as the intermediate nodes are trusted.
Details of the security of this network architecture are in
&lt;a href=&quot;#Salvail2010&quot;&gt;[5]&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/quantum/trusted_repeater_node_path.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;#Alleaume2014&quot;&gt;[2]&lt;/a&gt; “Hop-by-hop” transmission of the global key along a path in the network, encrypting/decrypting using local keys at each step. Each colored link represents an identical key pair shared between two neighbouring nodes that is used to encrypt messages between the nodes&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is important to note that the networks under consideration are
primarily for the purpose of key distribution. Any other communications
between the nodes is immaterial, and could be performed on any channel.
In order to simplify treatment and formulation of security guarantees,
the key distribution function of the network is entirely decoupled from
any other functions it may have. Only distribution is analyzed, under
the rationale that secure key distribution leads to provable information
theoretic communication using a one time pad.&lt;/p&gt;

&lt;h2 id=&quot;the-darpa-quantum-network&quot;&gt;The DARPA Quantum Network &lt;a href=&quot;#Elliott2005&quot;&gt;[1]&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The DARPA Quantum Network consists of 6 nodes operating through
telecommunications fiber between Harvard University, Boston University,
and BBN Technologies in Cambridge, and has been in continuous operation
since 2004. It is considered the first quantum cryptography network and
the first QKD systems operating continuously over a metropolitan area.&lt;/p&gt;

&lt;h3 id=&quot;systems&quot;&gt;Systems&lt;/h3&gt;

&lt;p&gt;The network uses four different kinds of hardware systems:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;BBN Mark 2 Weak-Coherent System: The core system transmits phase
modulated photons over telecommunications fiber, using a Mach
Zehnder interferometer to randomly modulate 1550nm laser pulses to
one of four phases, thereby encoding both a basis and a value. The
modulated pulses are augmented by bright pulses multiplexed over the
same fiber to provide timing and framing information. On the
receiving end, another interferometer randomly set to one of two
phases performs demodulation, followed by by routing to one of two
cooled InGaAs detectors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;BBN/BU Mark 1 Entangled System: A BB84-based system utilizing
polarization-entangled photon pairs that are transmitted over
telecommunications fiber. The basis value pairs needed for
transmission are encoded by polarization modulation, with random
basis selection performed by a beam splitter in a purely passive
fashion. Phase modulation is carried out using an external source of
randomness that drives carefully tuned interferometers. A key aspect
of the system is the incorporation of polarization controls in order
to mitigate the polarization scrambling effect of telecommunications
fiber. Errors are caused by significant attenuation resulting from
the interferometers and beam-splitters used, as well as the fiber
itself. Detector dark count introduces additional errors in the form
of spurious detection events, all of which contribute to an increase
in the Quantum Bit Error Rate of the system.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;NIST Freespace System &lt;a href=&quot;#Bienfang2004&quot;&gt;[6]&lt;/a&gt;: Four vertical-cavity
surface-emitting lasers (VCELs) are used to produce 250 picosecond
pulses with a high extinction ratio, providing the quantum sources
for the system. The pulses are then attenuated and coupled to
freespace optics, where they are collimated and lineary polarized
either vertically or in the $\pi/4$ direction, after which they are
shaped to fill the output aperture of the transmission telescope. At
the receiving end, an identical telescope receives the beam, passing
it through a non-polarizing beam-splitting cube that performs a
random choice of polarization basis, and then a polarizing
beam-splitting cube that measures the value of the polarization
through a fiber-couple detection box.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;QinetiQ Freespace System &lt;a href=&quot;#Tapster2005&quot;&gt;[7]&lt;/a&gt;: A BB84-based four-laser
faint-pulse transmission system is used to create the four
alternative polarization states, generating pulses at a rate of
10MHz. On the receiving side, measurement is performed automatically
using avalanche photodiodes (APDs), after which software mechanisms
perform sifting, error correction and privacy amplification to
generate a secure key. The key characteristics of the system are
compactness and portability, providing a final key exchange rate of
about 1kbit/s at a range of 40m.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;operation&quot;&gt;Operation&lt;/h3&gt;

&lt;p&gt;At a high level, the systems used in the network perform the following
basic functions in order to arrive at a shared key:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Sifting: Reconciliation of the raw secret bit streams as described
in the section on the BB84 protocol. This removes errors resulting
from failed detection (due to transmission losses, photons lost to
eavesdropping without replacement, or detector inefficiencies),
wrong basis choice (where Bob does in fact detect the transmitted
photon, but randomly selected the wrong basis in which to measure
it), or multiple detection events (where multiple detectors fire on
Bob’s end, as a result of which Bob cannot determine whether the
symbol transmitted is a one or a zero, and therefore must discard
it). This is done by public communication, after which only both
parties end up with highly correlated “sifted” bits. The DARPA
network implements both classic and SARG04 sifting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Error Detection and Correction: Elimination of bits damaged during
transmission, which is an inherently probabilistic process that
reveals information to an eavesdropper Eve. The end result is that
both parties end up with identical copies of a secret bit string
with high probability, about which an eavesdropper Eve has some
information. The QBER may also be estimated in this stage. The DARPA
network implements a modification of the Cascade protocol as well a
Forward Error Correction technique called Niagra &lt;a href=&quot;#Pearson2004&quot;&gt;[8]&lt;/a&gt; that
is designed by BBN. The Niagra protocol offers reduced
communications overhead, error correction delay, and CPU usage, at
the expense of a small decrease in coding efficiency.
&lt;a href=&quot;#Elliott2005&quot;&gt;[1]&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Entropy estimation: An accurate estimate of the amount of entropy in
the sifted bits, beyond what Eve might have information about, is a
necessary input to the privacy amplification step, and so is crucial
for the security of a QKD system. The DARPA network implements four
measures of this entropy, the details of which may be found in
&lt;a href=&quot;#Elliott2005&quot;&gt;[1]&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Privacy Amplification: As detailed in previous sections, this allows
Alice and Bob to reduce the amount of information gleaned by Eve
about their shared bits to some low acceptable level. The DARPA
network uses a linear hash function over the Galois Field $GF[2^n]$
(corresponding to polynomials with coefficient $0$ or $1$ and degree
less than $n$), where $n$ is the number of error-corrected bits in a
transmission. The QKD node that initiates amplification selects the
number of bits $m$ of the resulting hash, the primitive polynomial
of the field, a multiplier $n$ and an $m$ bit polynomial to add to
the product. Each side now has the information required to construct
the hash, which is truncated to $m$ bits and used to perform privacy
amplification.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Authentication: In this phase, Alice and Bob assure each other that
they are really exchanging information with each other, and not with
Eve, with high probability. The DARPA network uses the existing
authentication mechanisms of the Internet security architecture to
perform authentication. This relies on pre-shared secret keys,
although extensions using Universal Hashing are possible, which
allow continuous authentication using secret bits derived from
continuous QKD.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The DARPA network represented a huge step forward in the implementation
of quantum networks, showing that it is viable to conduct QKD in a
reliable, practically autonomous fashion over a relatively widespread
geographic area. The consolidation of multiple QKD technologies and the
close partnership between government agencies, research institutions,
and industry were a promising forerunner of the kind of progress to
expect in future years, and the promises made by the success of this
endeavor have been borne out in the subsequent proliferation of QKD
implementations, notably the SECOQC &lt;a href=&quot;#Peev2009&quot;&gt;[9]&lt;/a&gt; project.&lt;/p&gt;

&lt;h2 id=&quot;qkd-secured-bank-transfer&quot;&gt;QKD secured bank transfer &lt;a href=&quot;#Fedrizzi2005&quot;&gt;[10]&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;One of the most widely touted applications for a practical QKD system is
securing financial transfers. The first step in this direction was taken
in 2004, when researchers set up the first real-world application of an
entangled-state quantum cryptography protocol based on BB84. The
generated keys were used to secure an online wire transfer from the
Vienna City Hall to the headquarters of Bank-Austria Creditanstalt.&lt;/p&gt;

&lt;p&gt;Keeping in mind the theoretical vulnerability of pulse-based BB84
implementations to photon number splitting (PNS) attacks, the system in
question used entangled photon pairs &lt;a href=&quot;#Ekert1991&quot;&gt;[11]&lt;/a&gt;, in a modification of
the BB84. In such a system, the information to be transferred was stored
in correlations between the results of measurements on the individual
photons of the pair. Furthermore, the randomness of the generated key in
such a system arises from the quantum randomness of the measurement
itself, and does not rely on an external source of randomness, as
attenuated laser pulse systems do.&lt;/p&gt;

&lt;p&gt;The photon source was a compact device based on type-II spontaneous
parametric down-conversion, which produces entangled pairs with
orthogonal polarizations. The produced pair was then split up, with one
photon being sent to Alice directly, and the other sent to Bob through
1.45km of optical fiber specially installed through the Viennese sewer
system. The associated computations for key distillation were carried
out on a dedicated QKD hardware device &lt;a href=&quot;#Lieger2004&quot;&gt;[12]&lt;/a&gt;. Classical
communication was carried out over a TCP/IP connection through an
Ethernet bridge. The average QBER over the entire run time of the
experiment was less than 8%, of which 2.6% came from detection
imperfections and 1.2% came from imperfect production of the entangled
states, neither of which can be exploited by an eavesdropper and hence
can be excluded from the QBER factor in the calculations required for
privacy amplification. The resulting system had a raw transmission rate
of 80 bits/s after error correction and privacy amplification, which is
impressive for a first-use demo, but has been superseded by newer
technologies, as will be discussed in later sections.&lt;/p&gt;

&lt;p&gt;The experiment carried out here marks an important milestone in the
practical development of QKD, demonstrating the viability of real-time
quantum cryptography systems in a realistic environment. The success of
this work clearly foreshadows the later successes of quantum
cryptography, such as its use in the Geneva elections of 2007, discussed
later, and the increasingly ambitious projects involving the
establishment of QKD networks seen in later years.&lt;/p&gt;

&lt;h2 id=&quot;geneva-elections&quot;&gt;Geneva Elections&lt;/h2&gt;

&lt;p&gt;The 2007 Geneva elections saw the first commercial use of quantum
cryptography, where ID Quantique’s (IDQ) Cerberis system was used to
secure the transmission of vote counts from the central ballot-counting
station to the government data center &lt;a href=&quot;#Messmer07&quot;&gt;[13]&lt;/a&gt;. The version of
Cerberis used consisted of quantum key servers on both ends for key
generation, as well as a gigabit Ethernet link that carried the
encrypted transmissions &lt;a href=&quot;#Peck2007&quot;&gt;[14]&lt;/a&gt;. The keys generated by the quantum
link were used to transmit a 256-bit AES key, which was then used to
provide secure point-to-point communication between the counting station
and the data center by encrypting all messages between the two centers.
As an additional security guarantee, both the AES key and the quantum
key were renewed upto 60 times an hour in both directions &lt;a href=&quot;#IDQ10&quot;&gt;[15]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;secure-communications-based-on-quantum-cryptography&quot;&gt;Secure Communications based on Quantum Cryptography &lt;a href=&quot;#Peev2009&quot;&gt;[9]&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Between 2004 and 2008, 41 European research and industrial organizations
worked together with the goal of producing a scalable system of QKD with
an average link length of about 25km, demonstrating the practical
utility of the technologies developed so far that are capable of
supporting secure quantum transmissions, under a project titled Secure
Communications based on Quantum Cryptography (SECOQC). The highlight of
this work consists in the systematic development of a design that allows
unrestricted scalability and interoperability of QKD technologies. The
development of an internal communications standard (titled the Q3P)
communications interface was instrumental, allowing QKD devices to
communicate seamlessly with higher network layers. The entire
infrastructure of the project is based around a single modular design
utilizing the trusted repeater network paradigm.&lt;/p&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;

&lt;p&gt;Each node in the SECOQC architecture is built as the composition of two
modules, denoting a separation of concerns. A single node has many
functions; it must manage QKD keys generated over QKD links, ensure
encryption and decryption services for key transport across links,
communicate with other devices in a classical manner, manage keys
internally, and provide other cryptographic services (for key
distillation and privacy amplification, for instance). Given this, the
responsibilities of the node are split between the two modules, called
the &lt;em&gt;node module&lt;/em&gt; and the &lt;em&gt;QKD device&lt;/em&gt;. The node module performs the
network functions required, facilitating classical communications,
providing cryptography services, managing keys, etc.). It interfaces
with the QKD module to obtain the local QKD key that it then uses to
communicate securely with other nodes on the network. The QKD device has
the sole responsibility of communication over the quantum channel,
followed by key distillation and storing it in the classical node
module. This design allows for the use of arbitrary QKD devices, as long
as they adhere to a common interface detailed by the project, which
means that the network can be scaled up easily through the addition of a
variety of QKD devices. Another factor in the easy scaling of the
network is the fact that the number of keys that must be stored
increases linearly with the network size, as opposed to quadratically.
This is because, in the network graph, only neighbouring nodes need to
store link keys corresponding to the edge connecting them, as opposed to
every pair of nodes having to store keys in other network topologies.&lt;/p&gt;

&lt;p&gt;The SECOQC network itself consists of six nodes connected by eight
quantum links, with deployment taking place in 2008 and a public
demonstration taking place during a QKD conference in October, 2008. The
demo involved a one-time pad encrypted telephone communication, a secure
video conference involving all deployed nodes, and a number of rerouting
experiments. In line with the focus of SECOQC’s work, the various
transmissions that were part of the demo took place, strictly speaking,
over classical channels. The keys used to secure these channels were
derived using the QKD network, fulfilling the objective of the project
to create a robust, extensible quantum distribution network.&lt;/p&gt;

&lt;h3 id=&quot;systems-1&quot;&gt;Systems&lt;/h3&gt;

&lt;p&gt;Six different QKD systems were prototyped in Vienna as part of the
project:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Three upgraded ‘Cerberis’ systems from the Swiss company id
Quantique, which are plug and play pairs that can be used to set up
a QKD link. This system employs phase coding to carry out the BB84
protocol and the SARG &lt;a href=&quot;#Makoto2006&quot;&gt;[16]&lt;/a&gt; protocol (a modification of BB84
that is ideal for weak Poissonian sources). Multiple results testify
to high reliability of id Quantique systems, which was used for
ballot counting in the Swiss national elections of 2007, and have
been used for the same purpose in each election in the Geneva
canton. The performance of the upgraded system over a distance of
25km equated to a secret key rate of 1kbit/s.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One-way weak coherent pulse system with decoy states (Tosh) using a
phase-encoding QKD system with two interferometers, stabilized by
pulses that are time multiplexed with the quantum signals. The
protocol implemented is a weak coherent pulse (WCP) decoy state +
vacuum state BB84 variant, the details of which can be found in
&lt;a href=&quot;#Hwang2002&quot;&gt;[17]&lt;/a&gt;. The distinguishing feature is the use of decoy states
in order to estimate expected signal pulse losses, followed by
termination of the protocol if the estimated loss is much lower than
the experimentally observed loss. This is a remediation technique
for the photon-number-splitting attack against BB84, where an
eavesdropper surreptitiously diverts some fraction of the
transmitted photons. The key rate for a fiber length of 25km was
found to be 5.7 kbit/s, almost six times higher than the required
rate in the SECOQC specification. Furthermore, these rates are
almost a hundredfold improvement over BB84 without decoy pulses
&lt;a href=&quot;#Gobby2004&quot;&gt;[18]&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A coherent-one-way (COW) system that is an experimental realization
of distributed phase reference protocols. The COW protocol is
distantly related to the BB84 protocol, with the addition of a third
basis (corresponding to time-of-arrival)) and using one of the
original bases only to ensure coherence &lt;a href=&quot;#Lo2005&quot;&gt;[19]&lt;/a&gt;. At a high level,
Alice either sends pulses of weak coherent states or completely
blocks the beam (corresponding to vacuum pulses). Bob then uses an
interferometer and time-of-arrival measurements to distinguish
between bit values and check the coherence of each pulse. The
interferometer information provides indicates of eavesdropping.
Further details may be found in &lt;a href=&quot;#Stucki2005&quot;&gt;[20]&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A polarization entanglement QKD system (Ent) that supports
concurrent active stabilization of the optical elements for stable
long-term automated operation. This system implements the BBM92
&lt;a href=&quot;#Bennett1992&quot;&gt;[21]&lt;/a&gt; protocol, and includes numerous active automated
stabilization modules, allowing for completely autonomous startup
and continued uninterrupted service. A reliable key rate of about 2
kbit/s was observed for its operation during the SECOQC
demonstration, and a more nuanced discussion of the system may be
found in &lt;a href=&quot;#Treiber2009&quot;&gt;[22]&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A continuous variables (CV) system with Gaussian modulation, reverse
reconciliation and homodyne detection of the coherent light pulses.
The key information is stored in both quadratures of a coherent
state of the electromagnetic field, which can then be measured using
homodyne detection. The quadratures are simply the operators defined
by $\hat{p} = \frac{1}{\sqrt{2}}(\hat{a}^\dagger + \hat{a})$ and
$\hat{q} = \frac{1}{\sqrt{2}}(\hat{a}^\dagger - \hat{a})$, where
$\hat{a}$ and $\hat{a}^\dagger$ are the annihilation and creation
operators of the electromagnetic field. The process goes as follows;
Alice generates coherent laser pulses that are split into a weak
signal and a strong local oscillator (LO). The signal is then
randomly modulated according to a centered Gaussian distribution in
both quadratures. The LO and the signal are then time and
polarization multiplexed and transmitted to Bob on the same physical
optical fiber. At Bob’s end, the two signals are combined and
converted into an electric signal using a homodyne detector, where
the term homodyne simply refers to the fact that the LO and the
signal pulse are both derived from the same light source. The output
signal is proportional to the quadrature of the signal, which
depends on the phase difference between the LO and the signal. Bob
randomly picks either no phase difference or a difference of $\pi/2$
to select one of the two quadratures for measurement. Details of the
protocol may be found in &lt;a href=&quot;#Andersen2010&quot;&gt;[23]&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A free space link, using the BB84 protocol with decoy states
&lt;a href=&quot;#Hwang2002&quot;&gt;[17]&lt;/a&gt; and polarization encoded laser pulses.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusion-1&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The SECOQC network is a powerful testament to the progress made in the
practical implementation of QKD networks using the trusted repeater
paradigm. It serves as a landmark in paving the way for seamless
integration of QKD devices into higher level network layers through the
development of native protocols (Q3P, QKD-NL and QKD-TL) and by
exemplifying the performance, stability and robustness of a modular
architecture that separates QKD from classical networking functions. The
innovative core of the project can be boiled down to the concentration
of all QKD functions into a single node device, which makes it easy to
add links and grow the network when required, enhancing the scalability
properties of the network. Most importantly, the demonstration carried
out as part of the project shows that it is possible to perform every
day functions (telephony and video conferencing, for instance) in a
secure manner, building off of the inherent security of the QKD
paradigm.&lt;/p&gt;

&lt;h1 id=&quot;bibliography&quot;&gt;Bibliography&lt;/h1&gt;
&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;





&lt;span id=&quot;Elliott2005&quot;&gt;C. Elliott, A. Colvin, D. Pearson, O. Pikalo, J. Schlafer, and H. Yeh, “Current status of the DARPA Quantum Network,” pp. 1–12, 2005.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0304397514006963&quot;&gt;






&lt;span id=&quot;Alleaume2014&quot;&gt;R. Alléaume &lt;i&gt;et al.&lt;/i&gt;, “Using quantum key distribution for cryptographic purposes: A survey,” &lt;i&gt;Theor. Comput. Sci.&lt;/i&gt;, vol. 560, pp. 62–81, 2014.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Townsend1994&quot;&gt;P. D. Townsend, S. J. D. Phoenix, K. J. Blow, and S. M. Barnett, “Design of quantum cryptography systems for passive optical networks,” &lt;i&gt;Electron. Lett.&lt;/i&gt;, vol. 30, no. 22, p. 1875, 1994.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://www.nature.com/doifinder/10.1038/ncomms7787&quot;&gt;






&lt;span id=&quot;Azuma2015&quot;&gt;K. Azuma, K. Tamaki, and H.-K. Lo, “All-photonic quantum repeaters,” &lt;i&gt;Nat. Commun.&lt;/i&gt;, vol. 6, p. 6787, 2015.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;iospress.metapress.com/index/x340647040n00064.pdf\backslashnhttp://arxiv.org/pdf/0904.4072&quot;&gt;






&lt;span id=&quot;Salvail2010&quot;&gt;L. Salvail, M. Peev, E. Diamanti, and R. Alléaume, “Security of trusted repeater quantum key distribution,” &lt;i&gt;J. Comput. Secur.&lt;/i&gt;, vol. 18, no. 1, pp. 61–87, 2010.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Bienfang2004&quot;&gt;J. Bienfang &lt;i&gt;et al.&lt;/i&gt;, “Quantum key distribution with 1.25 Gbps clock synchronization.,” &lt;i&gt;Opt. Express&lt;/i&gt;, vol. 12, no. 9, pp. 2011–2016, 2004.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=864746&quot;&gt;






&lt;span id=&quot;Tapster2005&quot;&gt;P. R. Tapster, P. M. Gorman, D. M. Benton, D. M. Taylor, and B. S. Lowans, “Developments towards practical free-space quantum cryptography,” vol. 5815, pp. 176–179, 2005.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.1834439&quot;&gt;






&lt;span id=&quot;Pearson2004&quot;&gt;D. Pearson, “High-speed QKD Reconciliation using Forward Error Correction,” &lt;i&gt;AIP Conf. Proc.&lt;/i&gt;, vol. 734, no. 2004, pp. 299–302, 2004.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://iopscience.iop.org/1367-2630/11/7/075001&quot;&gt;






&lt;span id=&quot;Peev2009&quot;&gt;M. Peev, C. Pacher, and R. Alléaume, “The SECOQC quantum key distribution network in Vienna,” &lt;i&gt;New J. Phys.&lt;/i&gt;, vol. 11, p. 075001, 2009.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Fedrizzi2005&quot;&gt;a. Fedrizzi &lt;i&gt;et al.&lt;/i&gt;, “Practical quantum key distribution with polarization entangled photons,” &lt;i&gt;2005 Eur. Quantum Electron. Conf. EQEC ’05&lt;/i&gt;, vol. 2005, no. 16, p. 303, 2005.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Ekert1991&quot;&gt;A. K. Ekert, “Quantum Cryptography Based on Bell’s Theorem,” &lt;i&gt;Phys. Rev. Lett.&lt;/i&gt;, vol. 67, no. 6, pp. 661–663, 1991.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://cat.inist.fr/?aModele=afficheN&amp;amp;cpsidt=17633865\backslashnhttp%3A%2F%2Fwww.eurasip.org%2FProceedings%2FEusipco%2FEusipco2004%2Fdefevent%2Fpapers%2Fcr1391.pdf&amp;amp;ei=OXnTToS2B8Gp4gTvwNBS&amp;amp;usg=AFQjCNExLjbA_kMPxmUadP5IKmgrMiIDrA&quot;&gt;






&lt;span id=&quot;Lieger2004&quot;&gt;R. Lieger, T. Lorunser, G. Humer, and F. Schupfer, “Embedding quantum cryptography on DSP-boards,” &lt;i&gt;Proc. EUSIPCO-2004, 12th Eur. Signal Process. Conf.&lt;/i&gt;, pp. 2027–2030, 2004.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://www.networkworld.com/article/2286834/lan-wan/quantum-cryptography-to-secure-ballots-in-swiss-election.html&quot;&gt;






&lt;span id=&quot;Messmer07&quot;&gt;E. Messmer, “Quantum cryptography to secure ballots in Swiss election Advanced form of cryptography uses photons to exchange encryption keys,” &lt;i&gt;Netw. World&lt;/i&gt;, pp. 1–3, 2007.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://spectrum.ieee.org/computing/networks/geneva-vote-will-use-quantum-cryptography&quot;&gt;






&lt;span id=&quot;Peck2007&quot;&gt;M. E. Peck, “Geneva Vote Will Use Quantum Cryptography,” &lt;i&gt;IEEE Spectr.&lt;/i&gt;, pp. 1–2, 2007.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://www.idquantique.com/wordpress/wp-content/uploads/user-case-gva-gov.pdf&quot;&gt;






&lt;span id=&quot;IDQ10&quot;&gt;IQ Quantique, “Secure Data Transfer for Elections,” pp. 1–2, 2010.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://arxiv.org/abs/quant-ph/0603066&quot;&gt;






&lt;span id=&quot;Makoto2006&quot;&gt;E. Makoto, H. Manabu, and H. Imai, “A Quantum Key Distribution Protocol with Selecting Announced States, Robust against Photon Number Splitting Attacks,” p. 4, 2006.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://arxiv.org/abs/quant-ph/0211153&quot;&gt;






&lt;span id=&quot;Hwang2002&quot;&gt;W.-Y. Hwang, “Quantum Key Distribution with High Loss: Toward Global Secure Communication,” &lt;i&gt;Computer (Long. Beach. Calif).&lt;/i&gt;, no. August, p. 4, 2002.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Gobby2004&quot;&gt;G. Gobby, Z. L. Yuan, and A. J. Shields, “Unconditionally secure quantum key distribution over 50km of standard telecom fibre,” &lt;i&gt;Electron. Lett.&lt;/i&gt;, vol. 40, no. 25, pp. 1603–1605, 2004.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://www.springerlink.com/content/62ju52euu16uy6cn/&quot;&gt;






&lt;span id=&quot;Lo2005&quot;&gt;H.-K. Lo, H. F. Chau, and M. Ardehali, “Efficient Quantum Key Distribution Scheme and a Proof of Its Unconditional Security,” &lt;i&gt;J. Cryptol.&lt;/i&gt;, vol. 18, no. 2, pp. 133–165, 2005.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Stucki2005&quot;&gt;D. Stucki, N. Brunner, N. Gisin, V. Scarani, and H. Zbinden, “Fast and simple one-way quantum key distribution,” &lt;i&gt;Appl. Phys. Lett.&lt;/i&gt;, vol. 87, no. 19, pp. 1–3, 2005.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Bennett1992&quot;&gt;C. H. Bennett, G. Brassard, and N. D. Mermin, “Quantum cryptography without Bell’s theorem,” &lt;i&gt;Phys. Rev. Lett.&lt;/i&gt;, vol. 68, no. 5, pp. 557–559, 1992.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://arxiv.org/abs/0901.2725&quot;&gt;






&lt;span id=&quot;Treiber2009&quot;&gt;A. Treiber &lt;i&gt;et al.&lt;/i&gt;, “Fully automated entanglement-based quantum cryptography system for telecom fiber networks,” &lt;i&gt;New J. Phys.&lt;/i&gt;, vol. 11, no. 4, p. 20, 2009.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://arxiv.org/abs/1008.3468&quot;&gt;






&lt;span id=&quot;Andersen2010&quot;&gt;U. L. Andersen, G. Leuchs, and C. Silberhorn, “Continuous Variable Quantum Information Processing,” pp. 337–354, 2010.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Quantum Crypto Part 2</title>
   <link href="www.mnsgrg.com/2018/04/05/quantum-crypto-part-2/"/>
   <updated>2018-04-05T00:00:00-07:00</updated>
   <id>www.mnsgrg.com/2018/04/05/quantum-crypto-part-2</id>
   <content type="html">&lt;p&gt;This is the second in the Quantum Cryptography series of posts. This post
covers some of the theoretical underpinnings of QC, including some of
the earliest protocols and a metric for the error rate of
a quantum channel.&lt;/p&gt;

&lt;h2 id=&quot;the-bb84-protocol&quot;&gt;The BB84 Protocol &lt;a href=&quot;#Bennett1984&quot;&gt;[1]&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The BB84 protocol is widely accepted as the first quantum cryptography
protocol published. Its name comes from the initials of the authors of
the paper that first explained it (Bennett and Brassard), concatenated
with the year (1984) that the paper was published in. The protocol
relies on two fundamental tenets of quantum theory for its security; the
uncertainty theorem and the no-cloning theorem. The uncertainty theorem
extended from conjugate variables to conjugate bases implies that only
one bit of information to be derived from measurement on a single qubit
&lt;a href=&quot;#Wiesner1983&quot;&gt;[2]&lt;/a&gt;. More concretely, consider a qubit that has one of the
four following possible values:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{l l l l}
    \text{Bit 0:} &amp; \ket{\psi_0} &amp;= \ket{0} &amp; \\
    \text{Bit 1:} &amp; \ket{\psi_1} &amp;= \ket{1} &amp; \\
    \text{Bit 0:} &amp; \ket{\psi_2} &amp;= \ket{-} &amp;= \frac{1}{\sqrt{2}}(\ket{0}-\ket{1}) \\
    \text{Bit 1:} &amp; \ket{\psi_3} &amp;= \ket{+} &amp;= \frac{1}{\sqrt{2}}(\ket{0}+\ket{1}) \\
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;The true value of the qubit takes four possibilities, which means that
at least two bits of information are required to distinguish between the
possible values. First, a choice of basis must be made between the
computational basis $\left(\ket{0}, \ket{1}\right)$ vs. the Hadamard basis
$\left(\ket{+},\ket{-}\right)$. The second bit comes from a measurement made in the
correct basis which then tells us which of the basis states the qubit is
in. Suppose we take a qubit that is in the computational basis and
measure it in the Hadamard basis. Since the two bases are conjugate
&lt;a href=&quot;#Wiesner1983&quot;&gt;[2]&lt;/a&gt;, the result we get will be completely random; we
$\ket{0}$ half the time and $\ket{1}$ the other half. The same holds
true if we take a qubit in the Hadamard basis and measure it in the
computational basis. Therefore, choosing the wrong basis for measurement
completely destroys the information held by the qubit. This introduces
another possibility by which the information held by a qubit could be
extracted; we simply clone the qubit, and perform a measurement in the
computational basis on one copy, and a measurement in the Hadamard basis
on the other copy. This is impossible thanks to the no-cloning theorem,
and completes the intuition behind the foundations of the security of
the BB84 protocol.&lt;/p&gt;

&lt;p&gt;Alice begins by generating two random bit strings of length $n$, $a$ and
$b$. $a$ will form the basis for the shared secret, while $b$ will
decide which base to use in order to encode the current bit in $a$. We
index the strings from left to right in the binary representation, so
that $a_1$ is the leftmost bit in the representation of $a$. Now, Alice
sets up her machinery so as to enable her to transmit one of the four
qubits below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{l l l l}
    \text{Bit 0:} &amp; \ket{\psi_0} &amp;= \ket{0} &amp; \\
    \text{Bit 1:} &amp; \ket{\psi_1} &amp;= \ket{1} &amp; \\
    \text{Bit 0:} &amp; \ket{\psi_2} &amp;= \ket{-} &amp;= \frac{1}{\sqrt{2}}(\ket{0}-\ket{1}) \\
    \text{Bit 1:} &amp; \ket{\psi_3} &amp;= \ket{+} &amp;= \frac{1}{\sqrt{2}}(\ket{0}+\ket{1}) \\
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;For every index $i$ beginning at $1$ and ending at $n$, Alice transmits
the first basis vector if $a_i$ is zero and the second basis vector
otherwise. Furthermore, she transmits in the computational basis if
$b_i$ is zero and the Hadamard basis if $b_i$ is one. Put together, the
numbers formed by the bits $b_ia_i$ are precisely the index of the qubit
to use to encode the secret. An example transmission, with a random
short $a$ and $b$, along with the corresponding series of qubits is
shown below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    &amp; b = 0110101 \\
    &amp; a = 1011010 \\
    &amp; \psi_1\psi_2\psi_3\psi_1\psi_2\psi_1\psi_2\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Bob, on the other end of the transmission, generates his own random bit
string $c$ of length $n$. As he receives Alice’s $i’th$ transmission, he
measures it in the computational basis if $c_i$ is zero and the Hadamard
basis otherwise. In this manner, he destroys the information in the
qubit roughly half the time while accurately recovering it the other
half of the time. The amount of information Bob recovers is further
limited by transmission errors or inefficiencies in his detector.&lt;/p&gt;

&lt;p&gt;The next phase of the protocol occurs on a public communications channel
that protects against active eavesdropping. As the name implies, the
channel is completely open to passive eavesdropping, implying that the
adversary Eve knows everything that is sent over the public channel.
However, we assume that it is impossible for an adversary to insert
arbitrary messages or modify existing messages on the channel. To see
that this assumption is reasonable, consider the case where the public
channel is a medium such as a newspaper, and the information conveyed is
simply present in the form of an advertisement in the classifieds
section addressed to the receiver. For every qubit that is not lost in
transmission, Bob reports, over the public channel, the base used to
measure the value of that qubit. Alice replies with a confirmation every
time Bob reports the correct base, and a refutation when he does not.
Both Alice and Bob now know which qubits were transmitted accurately and
thus end up with a shared secret consisting of exactly those bits. Since
both Alice and Bob choose the qubit basis independently and at random,
there is a fifty percent chance that they choose the same basis for
qubit $i$, and so, ignoring transmission errors, about half the qubits
transmitted are correctly received by Bob, yielding a secret of length
about $n/2$. This process is known as sifting, and the generated key is
commonly called a sifted key, owing to obvious parallels between the
physical process of sifting physical mixtures to extract useful
components.&lt;/p&gt;

&lt;p&gt;Bennett and Brassard suggest sacrificing a third of the derived key bits
in order to guarantee security against eavesdropping in the following
manner: Alice and Bob publicly compare a third of the derived key bits,
keeping the remaining bits as valid key material only if they all agree.
To see why this works, consider the scenario with an eavesdropper
intercepting all of the $n$ qubits sent. Suppose Alice and Bob agree on
the basis used for $m$ out of the $n$ qubits sent. For each of the $m$
qubits, either Eve chose the wrong basis in which to intercept that
qubit, or she chose the correct basis. If she chose the correct basis,
both Alice and Bob’s value for the qubit’s state will agree. If she
chose the wrong basis, her first measurement destroyed the state of the
sent qubit, which means that the qubit she sent Bob has a random value
in the wrong basis. Since Bob used the same basis as Alice to measure
it, he used a basis conjugate to the basis Eve used, which means that
the value he measures is also random, and therefore matches Alice’s
value with probability $1/2$. Since Eve has $1/2$ probability of
choosing the right basis, the probability of both Alice and Bob agreeing
on the value of an arbitrary qubit from the $m$ they chose the same
basis for, is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{2} + \frac{1}{2} \times \frac{1}{2} = 3/4&lt;/script&gt;

&lt;p&gt;Here, the first term accounts for Eve choosing the correct basis while
the second term accounts for the times Bob randomly gets the correct
value even after Eve chooses the wrong basis. However, Eve only has
information about the key if she guesses the basis correctly and if Bob
measures the basis correctly (she doesn’t know the value of the qubit if
she guesses incorrectly, and the qubit isn’t used in the key if Bob
guesses incorrectly), which happens $1/4$ of the time. Therefore,
intercepting $n$ qubits gives Eve at most $n/4$ bits of information
about the key, while disrupting $1/4$ of the bits Alice and Bob agree
on, which is to say that Alice and Bob have different valued key bits
for $1/4$ of the bits that Bob used the same basis as Alice did, as a
result of eavesdropping. Therefore, for each derived key bit, the
probability that tampering goes undetected is $3/4$, which means that
comparing a third of the key bits gives you a probability of $P_f$ that
Eve succeeds in going undetected, where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_f = (3/4)^{c/3}&lt;/script&gt;

&lt;p&gt;Alice and Bob can choose $n$ and the fraction of $m$ to sacrifice to be
as large as required to satisfy an upper bound for the probability that
Eve gets away with eavesdropping undetected.&lt;/p&gt;

&lt;p&gt;In the general case, if Eve intercepts a fraction $\lambda$ of
transmitted qubits, the probability of one of the $m$ key qubits being
incorrect is $\lambda / 4$, where the factor $\lambda$ is the
probability of the chosen qubit to be intercepted, and $1/4$ is the
probability that Alice’s and Bob’s values for the qubit don’t match,
given that it is intercepted by $Eve$. Therefore, if a fraction $\eta$
of the key bits are sacrificed to check for eavesdropping, the
probability of a false negative (where eavesdropping goes undetected) is
simply $P_f’$, where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_f' = (1-\lambda/4)^{\eta m}&lt;/script&gt;

&lt;p&gt;Given that all goes well, at the end of the transmission, Alice and Bob
share a secret key of length $(1-\eta)m$ which can be used to
symmetrically encrypt further communications, e.g. using a one time pad.
This assumes that there are is no noise in the transmission. The case of
noisy transmissions can be analyzed using Quantum Error Correction
Codes, and yields a similar security guarantee. This high level overview
of the protocol shows how Alice and Bob can come to share some classical
information with a high degree of correlation. Accounting for
transmission errors and the fact that Eve obtains some information about
the shared secret, we see that this transmission is not perfect. The
work of Bennett, Brassard and Robert (&lt;a href=&quot;#Bennett1988&quot;&gt;[3]&lt;/a&gt;) outlines a process
they term Privacy Amplification, whereby classical discussion on the
public channel can be used to reduce the amount of information Eve has
about the shared secret, as well as to correct transmission errors. It
is this process that completes the key distribution, leaving Alice and
Bob with a perfectly correlated key with a high probability and reducing
the information leaked to Eve by an arbitrary amount.&lt;/p&gt;

&lt;h3 id=&quot;sarg04&quot;&gt;SARG04 &lt;a href=&quot;#Scarani2002&quot;&gt;[4]&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The preceding discussion of the BB84 protocol has focused on the single
photon picture. Single photon production and detection for quantum key
distribution remain technologically difficult, although it is the focus
of several ongoing research efforts with varying degrees of success,
with InGaAs based avalanche photodiodes being at the forefront
&lt;a href=&quot;#Comandar2014&quot;&gt;[5]&lt;/a&gt;. Most practical implementations of BB84 use weak laser
pulses in which Alice has encoded the bit to be sent, which introduces
the possibility of circumventing the guarantee provided by the
no-cloning theorem; Eve simply diverts some of the photons while
allowing the rest to proceed to Bob. Such an attack is known as a
photon-number splitting attack (PNS). If we consider an Eve to be
constrained only by the laws of physics, it is possible for her to store
the diverted photons in a quantum memory construct and wait until the
sifting phase, when Alice and Bob reveal the used bases, to measure the
stored photons in the correct base, obtaining full information about
them and precluding any process by which Bob and Alice can distill
completely secret keys. Moreover, Eve does not introduce any detectable
errors in this manner, resulting in insidiously successful leakage of
information.&lt;/p&gt;

&lt;p&gt;The extreme weakness of the protocol against this attack is due to the
orthogonality of paired states into which the bits are encoded; if Eve
knows the basis used, all she has to do is perform the appropriate
measurement to distinguish between the states, and therefore obtain the
transmitted bit. Intuitively, this can be circumvented by using
non-orthogonal states. Although the states are not orthogonal, one can
construct a measurement that distinguishes between the two states, with
the caveat that an inconclusive measurement is sometimes obtained. In
effect, a greater portion of the transmitted qubits must be satisfied
during sifting. For concreteness, a simple such protocol (from the
original paper by Scarani et al.) is outlined here.&lt;/p&gt;

&lt;p&gt;Alice randomly sends one of the four states $\ket{\pm x}$ or
$\ket{\pm z}$ to Bob, who randomly measures either in the $x$ basis
using $\sigma_x$ or the $z$ basis using $\sigma_z$. In this protocol,
successful transmission of $\ket{\pm x}$ corresponds to a 0, while
successful transmission of a $\ket{\pm z}$ corresponds to a 1. During
the sifting procedure, instead of revealing the bases, Alice announces
one of the four pairs of non-orthogonal states out of every possible
combination of sent photons, ${\ket{\pm x}} \times {\ket{\pm z}}$,
where the product is the cartesian product over the possible states.
This has the effect of limiting the possible valid measurements Bob can
make in the following manner: Suppose Alice announces
$\ket{+x}, \ket{+z}$. If Bob measured $\sigma_x$ and obtains the result
+1, he must discard the qubit, as such a result would have been possible
by measurement using $\sigma_z$ as well. Similarly, if he measures +1
using $\sigma_z$, he must discard the qubit, as it would have been
possible to obtain the same measurement using $\sigma_x$, and therefore
he cannot distinguish between the two possibilities. However, if he
measures $\sigma_z$ and obtains a result -1, he knows that the qubit
sent must have been in the state $\sigma_x$, as it would have been
impossible to obtain -1 using the $\sigma_z$ measurement, given that
Alice announces that she sent either $\ket{+x}$ or $\ket{+z}$.
Therefore, such a measurement allows Bob to keep the qubit, and add a
classical bit 1 to his sifted key. By symmetry, this leaves Bob with a
fourth of the raw key material after sifting, compared to the $1/2$
fraction obtained in classical BB84.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/quantum/compare.png&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;#Jeong2014&quot;&gt;[6]&lt;/a&gt; A comparison of secret key rate for BB84 (red) vs. SARG04
(blue)&lt;/p&gt;

&lt;p&gt;However, this results in a protocol that protects from PNS attacks by
significantly reducing the amount of information that Eve can obtain
about diverted photons. Scarani’s original paper &lt;a href=&quot;#Scarani2002&quot;&gt;[4]&lt;/a&gt; provides
a proof of this fact, calculating the information that Eve obtains in a
storage attack to be about $0.4$ bits per pulse at an attenuation rate
that allows her to keep one photon out of every pulse, compared to
complete leakage of information in the case of a storage attack on BB84.&lt;/p&gt;

&lt;h2 id=&quot;quantum-bit-error-rate&quot;&gt;Quantum Bit Error Rate &lt;a href=&quot;#Gisin2002&quot;&gt;[7]&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;A variety of environmental factors make it so that not all of the
photons transmitted by Alice are detected correctly by Bob. In practice,
some measure of reliability is required in order to characterize the
degree of losses induced by the environment. The quantum bit error rate
(QBER) provides this measure. It is defined, quite simply, as the ratio
of wrong bits to the total number of bits received, and is normally on
the order of a few percent. Note that this definition strictly only
applies to the BB84 protocol, and must be modified slightly for other
protocols.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{QBER} = \frac{N_{wrong}}{N_{right}+N_{wrong}} = \frac{R_{error}}{R_{sift}+R_{error}} \approx \frac{R_{error}}{R_{sift}}&lt;/script&gt;

&lt;p&gt;Clearly, the sifted rate is half the raw rate at which Bob detects
incoming photons, corresponding to the approximately one-half likelihood
that Alice and Bob pick compatible bases. The raw rate has four major
contributions; the pulse rate $f$, the mean number of photons per pulse,
$\mu$, the probability $t_{link}$ of the photons arriving at the
analyzer, and the probability $\eta$ of the photon being detected,
giving us that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{sift} = \frac{1}{2}R_{raw} = \frac{1}{2}f\mu t_{link}\eta&lt;/script&gt;

&lt;p&gt;Certain phase-coding setups introduce an additional factor $q$,
typically $1$ or $\frac{1}{2}$, which accounts for non-interfering path
combinations, which results in the following modification:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{sift} = \frac{1}{2}qf\mu t_{link}\eta&lt;/script&gt;

&lt;p&gt;Analyzing the error rate yields three contributions; photons that end up
in the wrong detector due to imperfections in the interferometer,
detector dark counts resulting from environmental light that is not
adequately filtered out, and uncorrelated photons due to imperfect
photon sources.&lt;/p&gt;

&lt;p&gt;The first, $R_{opt}$, is simply the sifted-key rate multiplied by the
probability that the photon goes to the wrong detector, $p_{opt}$. This
is clear, as the photon must have been transmitted successfully in order
for it to reach a detector, which is essentially the same as the
condition required for it to be part of the sifted key.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{opt} = p_{opt} R_{sift}&lt;/script&gt;

&lt;p&gt;The second, $R_{det}$, is independent of the bit rate, and only
contributes to the error rate if the dark count occurs during the time
when a photon is expected. Furthermore, the dark count occurs half the
time when Alice and Bob choose incompatible bases (in which case it does
not contribute to the error, as the corresponding detection is
eliminated during sifting) and an additional 50% chance of occurring in
the correct detector. If $p_{dark}$ is the probability of registering a
dark count per time window, we have the following expression:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{det} = \frac{1}{2} \frac{1}{2} f p_{dark}&lt;/script&gt;

&lt;p&gt;The third factor arises due to imperfect photon sources, where two
photons in different pairs arrive in the same time window, but not
necessarily in the same state. This is only relevant in systems using
entangled photons. If $p_{acc}$ is the probability of finding a second
photon within the time window belonging to a different pair than the
first photon detected, where the factors of $\frac{1}{2}$ are due to the
same reasons outlined before.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{acc} = \frac{1}{2} \frac{1}{2} p_{acc} f t_{link} \eta&lt;/script&gt;

&lt;p&gt;Therefore, the QBER is simply the ratio from earlier, with the
respective rates expanded into their separate contributions:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
    QBER &amp;= \frac{R_{error}}{R_{sift}}  \\
    &amp;= \frac{R_{opt} + R_{det} + R_{acc}}{R_{sift}} \\
    &amp;= p_{opt} + \frac{p_{dark}}{t_{link}\eta2q\mu} + \frac{p_{acc}}{2p\mu} \\
    &amp;= QBER_{opt} + QBER_{det} + QBER_{acc}\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The first contribution is entirely distance independent, and measures
the quality of the optical set up used. Different QC setups have
differing contributions and corresponding countermeasures to improve
$QBER_{opt}$, and this measure provides a single tool to evaluate the
quality of the optical set up alone.&lt;/p&gt;

&lt;p&gt;The second contribution is distance dependent, increasing with distance
as the probability of a photon arriving at the detector $t_{link}$ goes
down with increasing distance while the dark-count rate does not
fluctuate significantly. Therefore, issues with range are solely a
function of detector noise; better detectors allow for QC over much
higher distances.&lt;/p&gt;

&lt;p&gt;It is possible to calculate the maximum range of a QC system given these
parameters, where greater distances result in intolerably high values of
the QBER. Gisin&lt;a href=&quot;#Gisin2002&quot;&gt;[7]&lt;/a&gt; finds the maximum range to be about 100km,
practical maxima being closer to 50km owing to a variety of factors.&lt;/p&gt;

&lt;h1 id=&quot;bibliography&quot;&gt;Bibliography&lt;/h1&gt;
&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;
&lt;a href=&quot;https://www.cs.ucsb.edu/ chong/290N-W06/BB84.pdf&quot;&gt;






&lt;span id=&quot;Bennett1984&quot;&gt;C. H. Bennett and G. Brassard, “Quantum Cryptography: Public key distribution and coin tossing,” &lt;i&gt;Proc. IEEE Int. Conf. Comput. Syst. Signal Process. Bangalore&lt;/i&gt;. p. 175, 1984.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Wiesner1983&quot;&gt;S. Wiesner, “Conjugate Coding,” &lt;i&gt;ACM SIGACT News&lt;/i&gt;, vol. 15, no. 1, pp. 78–88, 1983.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Bennett1988&quot;&gt;C. H. Bennett, G. Brassard, and J.-M. Robert, “Privacy Amplification by Public Discussion,” &lt;i&gt;SIAM J. Comput.&lt;/i&gt;, vol. 17, no. 2, pp. 210–229, 1988.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://arxiv.org/abs/quant-ph/0211131 http://dx.doi.org/10.1103/PhysRevLett.92.057901&quot;&gt;






&lt;span id=&quot;Scarani2002&quot;&gt;V. Scarani, A. Acin, G. Ribordy, and N. Gisin, “Quantum cryptography protocols robust against photon number splitting attacks for weak laser pulses implementations,” vol. 1, no. 2, pp. 0–4, 2002.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Comandar2014&quot;&gt;L. C. Comandar &lt;i&gt;et al.&lt;/i&gt;, “Room temperature single-photon detectors for high bit rate quantum key distribution,” &lt;i&gt;Appl. Phys. Lett.&lt;/i&gt;, vol. 104, no. 2, 2014.&lt;/span&gt;



&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://stacks.iop.org/1612-202X/11/i=9/a=095201?key=crossref.b7833ea32bb584825d03d121f0102154&quot;&gt;






&lt;span id=&quot;Jeong2014&quot;&gt;Y.-C. Jeong, Y.-S. Kim, and Y.-H. Kim, “An experimental comparison of BB84 and SARG04 quantum key distribution protocols,” &lt;i&gt;Laser Phys. Lett.&lt;/i&gt;, vol. 11, no. 9, p. 095201, 2014.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;





&lt;span id=&quot;Gisin2002&quot;&gt;N. Gisin, G. Ribordy, W. Tittel, and H. Zbinden, “Quantum cryptography,” &lt;i&gt;Rev. Mod. Phys.&lt;/i&gt;, vol. 74, no. 1, pp. 145–195, 2002.&lt;/span&gt;



&lt;/li&gt;&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Quantum Crypto Part 1</title>
   <link href="www.mnsgrg.com/2018/03/13/quantum-crypto-part-1/"/>
   <updated>2018-03-13T00:00:00-07:00</updated>
   <id>www.mnsgrg.com/2018/03/13/quantum-crypto-part-1</id>
   <content type="html">&lt;p&gt;This is the first in a series of blog posts that make up my final report
for the Fall 2015 session of the Quantum Information and Quantum Computing
course offered at Georgia Tech. It is presented here as an reference for
theoretical Quantum Cryptography, as well as an increasingly
out of date survey of experimental methods in the field. This post introduces
the idea of Quantum Cryptography, and provides a brief overview of
traditional cryptography.&lt;/p&gt;

&lt;p&gt;It was Feynman who, in 1982, proposed the idea of a universal quantum
simulator &lt;a href=&quot;#Feynman1982&quot;&gt;[1]&lt;/a&gt;. The core idea was to use the inherent high
dimensionality of quantum mechanical systems in order to perform a large
number of computations in a short amount of time, which would allow such
simulators to trace the evolution of other quantum systems in a span of
time that could be considered practical. They can do this because of a
quantum mechanical concept superposition, wherein a single quantum
particle may be in two states at once. This means that a system of $n$
qubits can effectively occupy $2^n$ states, giving rise to the speedup
seen over classical computers. In this manner, the essential features of
quantum mechanics were shown to provide a way in which one could work
around the limitations imposed on us by quantum mechanics (the
difficulty of simulating quantum systems with and exponentially
increasing number of states).&lt;/p&gt;

&lt;p&gt;The world as we know it today relies heavily on cryptography. The
ubiquity of access to the internet combined with the requirements of
trust imposed by the need for secure multi-party communication across
the globe present a problem that is solved quite naturally using
techniques from classical cryptography. Most of these tools have at
their heart a reliance on the difficulty (under present conditions of
mathematical and computational understanding) of solving certain
problems; known formally as the cryptographic hardness assumption. Some
of these problems include scaled up versions of common exercises given
to children in middle school: factoring an integer into its constituent
primes. Others involve tasks in more esoteric mathematical settings;
finding $e^{th}$ roots of a given number modulo a large number $N$ (The
RSA Problem), or finding the logarithm of an element of a group with
respect to another element of the group as a base (The Discrete
Logarithm Problem). Each of these problems has been studied extensively,
to the point where we can confidently say that for certain reasonable
parameters of the above problem, it is infeasible for an computationally
limited adversary to solve the problem in a reasonable amount of time,
ensuring the security of the cryptographic system used&lt;a href=&quot;#Zhu2001&quot;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The development of fast quantum algorithms for a number of problems has
therefore posed an interesting challenge to the computational
assumptions made by cryptographers in analyzing cryptographic systems.
For instance, the RSA cryptosystem relies on the difficulty of finding
arbitrary roots of a number in a modular setting when the prime factors
of the modulus are not known. In general, this problem is easier than
integer factorization, which means that an efficient algorithm to
factorize integers will allow an efficient solution of the RSA problem -
it is almost trivial to find the required roots if the prime factors of
the modulus are known. Keeping this in mind, Shor’s success, in 1994, in
deducing a method by which an RSA modulus $N$ could be factored in
$O\left(lg(N)^{2+o(1)}\right)$ steps using a quantum computer of size
$O\left(lg(N)^{1+o(1)}\right)$ essentially sounded a death knell for the
RSA system&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;a href=&quot;#Shor1995&quot;&gt;[3]&lt;/a&gt;. It is enlightening, perhaps, to step back
for a moment here and note that practical quantum computers of
sufficient size, reliability, and cost parameters that would allow RSA
to be broken even by state level actors are quite far off, and that the
importance of this result is mostly theoretical at the current time. On
the other hand, the prevailing mood in the cryptographic community is to
jettison a system as soon as any chink is discovered in its armor, and
so a cautious move away from using RSA in production systems is well in
order. To illustrate the seriousness of this problem, between $17\%$ and
$26\%$ of the top one million websites on the internet (as of January,
2014) support some variant of RSA in their security suites.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;
Similarly, fast search algorithms like Grover’s eponymous construction
affect almost all cryptographic systems, although the degree to which
they are affected depends on the details of the system involved.
However, Compared to the massive speed up afforded by Shor’s algorithm
in the specific case of RSA, Grover’s algorithm is not nearly as
game-changing (It offers a search algorithm that takes
$\left(O\sqrt{N}\right)$ time, which, in practical terms, reduces the
effective security of the system used by half (Essentially making a
512-bit key only as secure as a 256-bit one)&lt;a href=&quot;#Grover1996&quot;&gt;[4]&lt;/a&gt;. Shor’s
algorithm, on the other hand, decimates the security properties of RSA,
requiring keys on the order of $2^{b/2}$ bits in length to provide the
same level of security that a $b$ bit key would provide in the absence
of Shor’s algorithm, which is an intolerable cost for any practical
application.&lt;/p&gt;

&lt;p&gt;In the theme of quantum phenomenon opening up new possibilities where it
closes old ones, the invalidation of a large portion of the existing
cryptographic infrastructure was somewhat alleviated by the discovery
that it was possible to use the quantum properties of microscopic
objects to transmit information between parties in a manner that is
essentially invulnerable to eavesdropping by a third party. The theory
surrounding this process will be the meat of this report, examining some
of the protocols that have been developed so far, the techniques used to
make them practical, and the security implications of some of the
features of quantum mechanics that allow us to do all of this with a
degree of certainty. Further, we examine experimental realizations of
the techniques described in the former half of the paper, providing a
measure of proof of the practicality of the notions covered. In
conclusion, we discuss some of the limitations of quantum channels and
possible directions future work in the area could take.&lt;/p&gt;

&lt;h1 id=&quot;history&quot;&gt;History&lt;/h1&gt;

&lt;p&gt;The story of cryptographic systems dates back to time immemorial. Humans
have kept secrets for as long as we’ve existed, and have tried to
communicate those secrets in a secure manner for as long as language
itself has existed. A number of solutions to this problem have arisen
over the years, although we shall skip most of the early development of
cryptosystems in favor of an overview of the modern cryptographic
infrastructure, beginning with a discussion of symmetric and asymmetric
cryptography.&lt;/p&gt;

&lt;p&gt;We will use the notions of plaintext (the unencrypted message to be
transmitted), ciphertext (the encrypted message), and key (some short
string that, when combined with the plaintext in a rigorous manner
specified by an encryption algorithm, produces the ciphertext)
throughout.&lt;/p&gt;

&lt;h2 id=&quot;symmetric-and-asymmetric-cryptography&quot;&gt;Symmetric and Asymmetric Cryptography&lt;/h2&gt;

&lt;p&gt;At its core, the distinction between symmetric and asymmetric
cryptography refers to the type and purpose of the keys used. Symmetric
cryptography (also called private key cryptography) is characterized by
the use of a single key for both encryption and decryption. This
generally leads to the operations of encryption and decryption having a
much simpler algorithmic structure and requiring relatively small keys.
Furthermore, the operations involved are generally simple computational
primitives like the XOR (Exclusive-OR) operator, or some permutations of
the input, or bit shifts, among others. Both these factors contribute to
the relative speed and minimal computational cost of symmetric
algorithm. However, this can only be used when the sender and receiver
possess the same shared secret key before secure communication begins.
They may do this in a number of ways; physically meeting up to exchange
keys, sending keys by post, or using some other protocol to exchange
keys. This struggle to exchange keys is a microscopic reflection of the
larger problem of secure communication that cryptography tries to solve.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/quantum/symmetric.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;#MicrosoftCorporation2005a&quot;&gt;[5]&lt;/a&gt; The process of symmetric encryption&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Asymmetric cryptography (also called public-key cryptography) involves
two keys; a public key and a private key. As the names suggest, the
public key is distributed publicly. It is not to be kept secret, and
anyone can gain access to it. The private key, on the other hand, must
be guarded at all times, as the security of the entire process relies on
this guarantee. There are two modes of operation, both of which have
their uses.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;In the first mode, the public key is used to encrypt a message,
which can then only be decrypted by the corresponding private key.
This ensures that anybody can send a message securely to the holder
of a particular private key, and rest assured that an eavesdropper
who intercepts the message will not be able to extract useful
information from it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The second mode of operation is useful when you wish to ensure that
a particular message comes from a particular person; a signature of
sorts. Here, the private key is used to encrypt a message which can
only be decrypted by the corresponding public key. If you are able
to use Alice’s public key to decrypt a message sent to you, that
acts as a guarantee that the message was sent by Alice, since nobody
else has Alice’s private key, and hence would not be able to
generate a ciphertext that can be decrypted by Alice’s public key.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The algorithms used to implement asymmetric cryptographic protocols make
use of some rather heavy mathematical machinery, as a result of which
the techniques they give rise to are computationally more complex than
the techniques of symmetric cryptography. They are, therefore, more
useful when the messages involved are short in length.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/quantum/asymmetric.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;#MicrosoftCorporation2005a&quot;&gt;[5]&lt;/a&gt; The process of asymmetric encryption&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This leads to a natural use of asymmetric algorithms in solving the
problem of key exchange we encountered above. Consider two parties,
Alice and Bob, who wish to communicate securely. Both parties have their
respective private keys $K_A$ and $K_B$, as well as their public keys
$P_A$ and $P_B$. It would be inefficient to directly use the
participants’ public and private keys to encrypt all communications
between them in an asymmetric manner, owing to the relatively large
computational cost of doing so. However, Alice can create a short random
string $S$, which she encrypts using her private key and sends to Bob.
Bob, who has Alice’s public key, can use it to decrypt Alice’s message
and recover the string $S$, which they can then use as a shared private
key to encrypt subsequent communications in an efficient manner using a
symmetric encryption algorithm. This description offers a high level
view of how symmetric and asymmetric algorithms work together to form
the basis of secure communications today, although a number of details
are absent (in particular, the problems of authenticating the
participants and of message integrity are not considered here, although
they are solved problems with well-known protocols).&lt;/p&gt;

&lt;h2 id=&quot;one-time-pad&quot;&gt;One-Time Pad&lt;/h2&gt;

&lt;p&gt;In most discussion of cryptography, the one-time pad is introduced
before more sophisticated ciphers, simply because it is considered more
fundamental, and because it’s operation is much simpler than the methods
discussed above. Here, it seems appropriate to discuss the one-time pad
and its security implications after discussing other ciphers, as it
allows for greater flexibility in contrasting the security properties of
the two. The discussion of the one-time pad is simplest in the case
where messages are simply bit streams of some length $n$. The
ciphertext, the plaintext, and the key are all of this form. Given this,
the key for a one-time pad is simply a perfectly random bit stream of
length $n$. The encryption procedure consists of generating, bit-by-bit,
the bitwise sum of the plaintext with the corresponding bit in the key,
a process that is equivalent to the bitwise addition modulo 2 of the two
bit streams. Given the random nature of the key, this is equivalent to
randomly flipping bits of the plaintext. To make the procedure clearer,
an example plaintext, key, and corresponding ciphertext are shown below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
        \text{Plaintext }  &amp; 0100101001011101110101 \\
        \text{Key }        &amp; 1101101000110100110101 \\
        \text{Ciphertext } &amp; 1001000001101001000000
    \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Decryption is just as simple; the exact same process is performed, using
the ciphertext and key as input this time. The preceding ciphertext is
reused in the following example:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
        \text{Key }        &amp; 1101101000110100110101 \\
        \text{Ciphertext } &amp; 1001000001101001000000 \\
        \text{Plaintext }  &amp; 0100101001011101110101 
    \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;At a high level, the security of the above scheme relies on the fact
that given a ciphertext $m$, it is possible that the ciphertext was
generated by any possible plaintext. Under the appropriate choice of
key, any plaintext encrypts to $m$. In fact, the required key is simply
the bitwise addition modulo 2 of the ciphertext and the target
plaintext. Therefore, there is no reason to prefer one plaintext over
another; every possible plaintext is equally likely to give rise to $m$
under the appropriate choice of key. An adversary that possesses only
the ciphertext has, in information-theoretic terms, no additional
advantage; he knows no more about the plaintext than he did before he
obtained the ciphertext. This property is formalized in a notion called
perfect secrecy, which only one time pads are have, as proved by Shannon
in his seminal paper &lt;a href=&quot;#Shannon1949&quot;&gt;[6]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The key property of the one-time pad, then, is this: the one-time is
&lt;em&gt;provably secure&lt;/em&gt;, a property not shared by any other encryption scheme.
The most we can say about the symmetric and asymmetric methods
introduced above is that they have not yet been proven insecure, a very
important distinction, which leaves room for mathematical or other
theoretical breakthroughs to find insecurities in them. If mathematical
advances were to render trivial the problems whose difficulty public key
cryptography relies upon, for instance, all of the current key
distribution architecture would be rendered useless. The one-time pad is
similar to quantum key distribution in that when carried out properly,
quantum key distribution is provably secure; it is physically impossible
for an eavesdropper to obtain information about the derived shared key.
Of course, both security properties (in the one-time pad case as well as
the QKD case) rely on the correctness of the implementation of the
scheme. Mistakes in carrying out either protocol leave room for
disastrous consequences, in some cases completely destroying the safety
of the system used. For instance, a cardinal mistake in implementing a
one-time pad is key reuse. If the same key is used to encrypt multiple
messages, it becomes possible for an adversary in possession of multiple
ciphertexts encrypted with the same key to derive the key, breaking the
system completely. The security guarantee, then, rests on the single-use
property of keys. In practice, this requirement, combined with the fact
that one-time pad keys are as long as the message themselves, present an
almost insurmountable obstacle to their widespread use. If Alice and Bob
had some secure mechanism by which they could exchange one-time pad
keys, they could just as easily exchange the message itself, since the
key is just as long as the message. This is why, traditionally,
asymmetric key cryptography is implemented; it can be leveraged for key
exchange, after which the exchanged keys can be utilized in a symmetric
key scheme for secure communication. QKD provides a solution to this
quandary, enabling provably secure key exchange, which can then be used
to encrypt messages securely using the one-time pad. The combination of
QKD and one-time pads provides a provably secure form of encryption,
something previously thought to be a mere pipe dream and standing as an
ultimate stronghold of encryption that will weather theoretical
advances, whether mathematical or computational.&lt;/p&gt;

&lt;h1 id=&quot;bibliography&quot;&gt;Bibliography&lt;/h1&gt;
&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;
&lt;a href=&quot;https://people.eecs.berkeley.edu/\~christos/classics/Feynman.pdf&quot;&gt;






&lt;span id=&quot;Feynman1982&quot;&gt;R. P. Feynman, “Simulating physics with computers,” &lt;i&gt;Int. J. Theor. Phys.&lt;/i&gt;, vol. 21, no. 6-7, pp. 467–488, 1982.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://digitool.library.mcgill.ca/R/?func=dbin-jump-full&amp;amp;object_id=33866&amp;amp;local_base=GEN01-MCG02&quot;&gt;






&lt;span id=&quot;Zhu2001&quot;&gt;H. Zhu, “Survey of computational assumptions used in cryptography broken or not by Shor’s algorithm,” 2001.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://arxiv.org/abs/quant-ph/9508027&quot;&gt;






&lt;span id=&quot;Shor1995&quot;&gt;P. W. Shor, “Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer,” &lt;i&gt;SIAM J. Sci. Stat. Comput.&lt;/i&gt;, vol. 26, p. 1484, 1995.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://arxiv.org/quant-ph/9605043v3&quot;&gt;






&lt;span id=&quot;Grover1996&quot;&gt;L. K. Grover, “A fast quantum mechanical algorithm for database search,” &lt;i&gt;Proceedings, 28th Annu. ACM Symp. Theory Comput.&lt;/i&gt;, pp. 212–219, 1996.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://msdn.microsoft.com/en-us/library/ff650720.aspx&quot;&gt;






&lt;span id=&quot;MicrosoftCorporation2005a&quot;&gt;Microsoft Corporation, “Data Confidentiality,” vol. 650720. pp. 1–4, 2005.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;a href=&quot;http://netlab.cs.ucla.edu/wiki/files/shannon1949.pdf&quot;&gt;






&lt;span id=&quot;Shannon1949&quot;&gt;C. E. Shannon, “Communication theory of secrecy systems,” &lt;i&gt;Bell Syst. Tech. J.&lt;/i&gt;, vol. 28, no. July, pp. 656–715–656–715, 1949.&lt;/span&gt;



&lt;/a&gt;

&lt;/li&gt;&lt;/ol&gt;

&lt;h1 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h1&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;The notation $o(1)$ suggests a quantity that tends to $0$ as the
input size increases &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;As reported by Vehent, J. here:
&lt;a href=&quot;https://jve.linuxwall.info/blog/index.php?post/TLS_Survey&quot;&gt;https://jve.linuxwall.info/blog/index.php?post/TLS_Survey&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Xavier Initialization</title>
   <link href="www.mnsgrg.com/2017/12/21/xavier-initialization/"/>
   <updated>2017-12-21T00:00:00-08:00</updated>
   <id>www.mnsgrg.com/2017/12/21/xavier-initialization</id>
   <content type="html">&lt;p&gt;This post assumes that you know enough about neural networks to follow through some math involving the backpropagation equations. For the most part, I only do basic algebraic manipulations, with a small dusting of basic statistics thrown in. If you know nothing about neural nets and have an hour or so to spare, the excellent &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/&quot;&gt;Neural Networks and Deep Learning&lt;/a&gt; is a good place to learn the basics, and getting as far as Chapter 2 should teach you enough to follow the math here. I try here to flesh out some of the math Glorot and Bengio skipped in their paper about initializing weights in deep neural networks, to better illuminate the intution behind why their method solves a longstanding problem facing the training of such networks.&lt;/p&gt;

&lt;h2 id=&quot;vanishing-gradients&quot;&gt;Vanishing Gradients&lt;/h2&gt;
&lt;p&gt;Initially, one of the challenges preventing the efficient training of very deep neural networks was the phenomenon of extreme gradients. If you look at a plot of the sigmoid activation function, a common choice in vanilla neural networks, it is clear that the derivative approaches zero at the extremes of the function; activations close to 1 or close to 0 both produce very small gradients. A neuron is said to be saturated when its activation occupies these extreme regimes. It was observed during the training of very deep networks that the last hidden layer would often quickly saturate to 0, causing the gradients to be close to 0 as well, which led to the backpropagated gradients in each preceding layer to become smaller and smaller still, until the very first hidden layers felt almost no change to their weights at all from the almost-zero gradients. This is clearly disastrous; the earlier hidden layers are supposed to be busy identifying features in the dataset that successive layers can then use to build more complex features at an even high level of abstraction. If the gradients reaching these early layers do not affect their weights, they end up learning nothing from the dataset, with predictable results on model accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/sigmoid.png&quot; alt=&quot;sigmoid&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this plot of the sigmoid activation function (the blue line), and its derivative (red line), you can clearly see the regions of saturation (light red background), where forcing the function to go to zero also forces its derivative to go to zero, causing the vanishing gradients issue as you move back through the layers.&lt;/p&gt;

&lt;h2 id=&quot;glorot-and-bengio&quot;&gt;Glorot and Bengio&lt;/h2&gt;
&lt;p&gt;Xavier Glorot and Yoshua Bengio examined the theoretical effects of weight initialization on the vanishing gradients problem in their 2010 paper&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. The first part of their paper compares activation functions, explaining how certain peculiarities of the commonly-used sigmoid function make it highly susceptible to the problem of saturation, and showing that the hyperbolic tangent and softsign &lt;script type=&quot;math/tex&quot;&gt;\left( \frac{x}{1 + |x|} \right)&lt;/script&gt; activations perform better in this respect.&lt;/p&gt;

&lt;p&gt;The second part of their paper considers the problem of initializing weights in a fully connected network, providing theoretical justification for sampling the initial weights from the uniform distribution of a certain variance. The motivating intuition for this is in two parts; for the forward pass, ensuring that the variance of the activations is approximately the same across all the layers of the network allows for information from each training instance to pass through the network smoothly. Similarly, considering the backward pass, relatively similar variances of the gradients allows information to flow smoothly backwards. This ensures that the error data reaches all the layers, so that they can compensate effectively, which is the whole point of training.&lt;/p&gt;

&lt;h2 id=&quot;notation-and-assumptions&quot;&gt;Notation and Assumptions&lt;/h2&gt;
&lt;p&gt;In order to formalize these notions, first we must get some notation out of the way. We have the following definitions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;a^L&lt;/script&gt; is the activation vector for layer &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;, with dimensions &lt;script type=&quot;math/tex&quot;&gt;n_L \times 1&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;n_L&lt;/script&gt; is the number of units in layer &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;W^L&lt;/script&gt; is the matrix of weights for layer &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;, with dimensions &lt;script type=&quot;math/tex&quot;&gt;n_L \times n_{L-1}&lt;/script&gt;. Each element &lt;script type=&quot;math/tex&quot;&gt;W^L_{jk}&lt;/script&gt; represents the weight of the connection from neuron &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; of the current layer to neuron &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; of the previous one.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;b^L&lt;/script&gt; is the bias vector for layer &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;, with the same dimensions as &lt;script type=&quot;math/tex&quot;&gt;a^L&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;z^L&lt;/script&gt; is the weighted input to the activation function of layer &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;. This means that &lt;script type=&quot;math/tex&quot;&gt;z^L = W^L \times a^{L-1} + b^L&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; is the cost function we’re trying to optimize. Glorot and Bengio use the conditional log likelihood &lt;script type=&quot;math/tex&quot;&gt;-\log P(y \vert x)&lt;/script&gt;, although the details of this won’t matter much.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; is the activation function, so that &lt;script type=&quot;math/tex&quot;&gt;a^L = \sigma (z^L)&lt;/script&gt;, where the function is applied to each element of the vector.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;n_L&lt;/script&gt; is the number of units in layer &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is the input vector to the network.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\delta^L = \frac{\delta C}{\delta z^L}&lt;/script&gt; is the gradient of the cost function w.r.t. the weighted inputs of layer &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;, also called the error.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following analysis holds for a fully connected neural network with &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; layers, with a symmetric activation function with unit derivative at zero. The biases are initialized to zero, and the activation function is approximated by the identity &lt;script type=&quot;math/tex&quot;&gt;f(x) = x&lt;/script&gt; for the initialization period.&lt;/p&gt;

&lt;p&gt;We assume that the weights, activations, weighted inputs, raw inputs to the network, and the gradients all come from independent distributions whose parameters depend only on the layer under consideration. Under this assumption, the common scalar variance of the weights of layer &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; is represented by &lt;script type=&quot;math/tex&quot;&gt;\text{Var}\left[W^L\right]&lt;/script&gt;, with similar representations for the other variables (activations, gradients, etc.)&lt;/p&gt;

&lt;!-- Insert image of a fully connected 3 layer net illustrating the notation --&gt;

&lt;h2 id=&quot;forward-pass&quot;&gt;Forward pass&lt;/h2&gt;

&lt;p&gt;For the forward pass, we want the layers to keep the input and output variances of the activations equal, so that the activations don’t get amplified or vanish upon successive passes through the layers. Consider &lt;script type=&quot;math/tex&quot;&gt;z^L_j&lt;/script&gt;, the weighted input of unit &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; in layer &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
 \text{Var}\left[z^L\right] &amp;= \text{Var}\left[z^L_j\right] \\
 &amp;= \text{Var}\left[ \sum_{k=0}^{n_{L-1}} W^L_{jk} a^{L-1}_k + b^L_j  \right] \\
 &amp;= \text{Var}\left[ \sum_{k=0}^{n_{L-1}} W^L_{jk} a^{L-1}_k \right] \\
 &amp;= \sum_{k=0}^{n_{L-1}} \text{Var}\left[ W^L_{jk} a^{L-1}_k \right] \\
 &amp;= \sum_{k=0}^{n_{L-1}} \text{Var}\left[ W^L_{jk}\right] \text{Var}\left[a^{L-1}_k \right] \\
 &amp;= \sum_{k=0}^{n_{L-1}} \text{Var}\left[ W^L \right] \text{Var}\left[a^{L-1} \right] \\
 &amp;= n_{L-1} \text{Var}\left[ W^L \right] \text{Var}\left[a^{L-1} \right] \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the above simplification, we use the fact that the variance of the sum of two independently random variables is the sum of their variances, under the assumption that the weighted activations would be independent of each other. Later, the variance of the product was expanded out to the product of the variances under the assumption that the weights of the current layer would be independent of the activations of the previous layer. The full expression of the variance of the product of two independent random variables also includes terms containing their means. However, we assume that both activations and weights come from distributions with zero mean, reducing the expression to the product of the variances.&lt;/p&gt;

&lt;p&gt;Since the activation function is symmetric, it has value 0 for an input of 0. Furthermore, given that the derivative at 0 is 1, we can approximate the activation function &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; as the identity during initialization, where the biases are zero and the expected value of the weighted input is zero as well. Under this assumption, &lt;script type=&quot;math/tex&quot;&gt;a^{L-1} \approx z^{L-1}&lt;/script&gt;, which reduces the previous expression to the form of a recurrence:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
 \text{Var}\left[z^L\right] &amp;= n_{L-1} \text{Var}\left[ W^L \right] \text{Var}\left[a^{L-1} \right] \\
 &amp;= n_{L-1} \text{Var}\left[ W^L \right] \text{Var}\left[z^{L-1} \right] \\
 &amp;= n_{L-1} \text{Var}\left[ W^L \right] n_{L-2} \text{Var}\left[ W^{L-1} \right] \text{Var}\left[z^{L-2} \right] \\
 &amp;= \text{Var}\left[ x \right] \prod_{m=0}^{L-1} n_m \text{Var}\left[ W^{m+1} \right]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus, if we want the variances of all the weighted inputs to be the same, the product term must evaluate to &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;, the easiest way to ensure which is to set &lt;script type=&quot;math/tex&quot;&gt;\text{Var}\left[ W^{m+1} \right] = \frac{1}{n_m}&lt;/script&gt;. Written alternatively, for every layer &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;n_\text{in}&lt;/script&gt; is the number of units to the layer (layer fan-in), we want&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Var}\left[ W^{L} \right] = \frac{1}{n_\text{in}}&lt;/script&gt;

&lt;h2 id=&quot;backwards-pass&quot;&gt;Backwards pass&lt;/h2&gt;
&lt;p&gt;For the backward pass, we want the variances of the gradients to be the same across the layers, so that the gradients don’t vanish or explode prematurely. We use the backpropagation equation as our starting point:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
 \text{Var}\left[\delta^L\right] &amp;= \text{Var}\left[\delta^L_j\right] \\
 &amp;= \text{Var}\left[ \left(\sum_{k=0}^{n_{L+1}} W^{L+1}_{kj} \delta^{L+1}_k \right) \sigma'(z^L_j) \right] \\
 &amp;= \text{Var}\left[ \sum_{k=0}^{n_{L+1}} W^{L+1}_{kj} \delta^{L+1}_k \right] \\
 &amp;= \sum_{k=0}^{n_{L+1}} \text{Var}\left[ W^{L+1}_{kj} \delta^{L+1}_k \right] \\
 &amp;= \sum_{k=0}^{n_{L+1}} \text{Var}\left[ W^{L+1}_{kj} \right] \text{Var}\left[ \delta^{L+1}_k \right] \\
 &amp;= \sum_{k=0}^{n_{L+1}} \text{Var}\left[ W^{L+1} \right] \text{Var}\left[ \delta^{L+1} \right] \\
 &amp;= n_{L+1} \text{Var}\left[ W^{L+1} \right] \text{Var}\left[ \delta^{L+1} \right] \\
 &amp;= n_{L+1} \text{Var}\left[ W^{L+1} \right] n_{L+2} \text{Var}\left[ W^{L+2} \right] \text{Var}\left[ \delta^{L+2} \right] \\
 &amp;= \text{Var}\left[ \delta^d \right]\prod_{m=L+1}^{d-1} n_{m} \text{Var}\left[ W^{m} \right] \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Similarly as in the forwards pass, we assume that the gradients are independent of the weights at initilization, and use the variance identities as explained before. Additionally, we make use of the fact that the weighted input &lt;script type=&quot;math/tex&quot;&gt;z^L&lt;/script&gt; has zero mean during the initialization phase in approximating the derivative of the activation function &lt;script type=&quot;math/tex&quot;&gt;\sigma'(z^L_j)&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;. In order to ensure uniform variances in the backwards pass, we obtain the constraint that &lt;script type=&quot;math/tex&quot;&gt;\text{Var}\left[W^m\right] = \frac{1}{n_m}&lt;/script&gt;, which can be written in the following form for every layer &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; and layer fan-out &lt;script type=&quot;math/tex&quot;&gt;n_\text{out}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Var}\left[ W^{L} \right] = \frac{1}{n_\text{out}}&lt;/script&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In the general case, the fan-in and fan-out of a layer may not be equal, and so as a sort of compromise, Glorot and Bengio suggest using the average of the fan-in and fan-out, proposing that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Var}\left[ W^{L} \right] = \frac{2}{n_\text{out} + n_\text{in}}&lt;/script&gt;

&lt;p&gt;If sampling from a uniform distribution, this translates to sampling the interval &lt;script type=&quot;math/tex&quot;&gt;[-a,a]&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;a = \sqrt{\frac{6}{n_\text{out} + n_\text{in}}}&lt;/script&gt;. The weird-looking &lt;script type=&quot;math/tex&quot;&gt;\sqrt{6}&lt;/script&gt; factor comes from the fact that the variance of a uniform distribution over the interval &lt;script type=&quot;math/tex&quot;&gt;[-a,a]&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;a^2/3&lt;/script&gt;. Alternatively, the weights can be sampled from a normal distribution with zero mean and variance same as the above expression.&lt;/p&gt;

&lt;p&gt;Before this paper, the accepted standard initialization technique was to sample the weights from the uniform distribution over the interval &lt;script type=&quot;math/tex&quot;&gt;\left[-\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}} \right]&lt;/script&gt;, which led to the following variance over the weights: &lt;script type=&quot;math/tex&quot;&gt;\text{Var}\left[ W^L \right] = \frac{1}{3n^L}&lt;/script&gt;. Plugging this into the equations we used for the backward pass, it is clear that the gradients decrease as we go backwards through the layers, reducing by about &lt;script type=&quot;math/tex&quot;&gt;1/3^\text{rd}&lt;/script&gt; at each layer, an effect that is borne out experimentally as well. The paper found that the new initialization method suggested ensured that the gradients remained relatively constant across the layers, and this method is now standard for most Deep Learning applications.&lt;/p&gt;

&lt;p&gt;It is interesting that the paper makes the assumption of a symmetric activation function with unit derivative at zero, neither of which conditions is satisfied by the logistic activation function. Indeed, the experimental results in the paper (showing unchanging gradients across layers with the new initialization method) are shown with the &lt;script type=&quot;math/tex&quot;&gt;tanh&lt;/script&gt; activation function, which satisfies both assumptions.&lt;/p&gt;

&lt;p&gt;For activation functions like ReLU, He et al. work out the required adjustments in their paper&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. At a high level, since the ReLU function basically zeroes out an entire half of the domain, it should suffice to compensate by doubling the variance of the weights, a heuristic that matches the result of He’s more nuanced analysis, which suggests that &lt;script type=&quot;math/tex&quot;&gt;\text{Var}\left[ W^{L} \right] = \frac{4}{n_\text{out} + n_\text{in}}&lt;/script&gt; works well.&lt;/p&gt;

&lt;h3 id=&quot;logistic-activation&quot;&gt;Logistic Activation&lt;/h3&gt;
&lt;p&gt;In the forward pass derivation, we approximate the activation function as approximately equal to the identity in the initialization phase we are interested in. For the logistic activation function, the equivalent approximation works out to be &lt;script type=&quot;math/tex&quot;&gt;\frac{x}{4} + \frac{1}{2}&lt;/script&gt; (since the derivative at zero is &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{4}&lt;/script&gt; and the value of the function at zero is &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{2}&lt;/script&gt;), using a truncated Taylor series expansion around 0. Plugging this in,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
 \text{Var}\left[z^L\right] &amp;= n_{L-1} \text{Var}\left[ W^L \right] \text{Var}\left[a^{L-1} \right] \\
 &amp;= n_{L-1} \text{Var}\left[ W^L \right] \text{Var}\left[\frac{z^{L-1}}{4} + \frac{1}{2} \right] \\
 &amp;= n_{L-1} \text{Var}\left[ W^L \right] \text{Var}\left[\frac{z^{L-1}}{4} \right] \\
 &amp;= \frac{n_{L-1}}{16} \text{Var}\left[ W^L \right] \text{Var}\left[z^{L-1} \right] \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The remaining steps are identical, except for the factor &lt;script type=&quot;math/tex&quot;&gt;1/16&lt;/script&gt; in front.&lt;/p&gt;

&lt;p&gt;Similarly in the backward pass, where we ignored the derivative of the activation function under the assumption that it was zero, plugging in the correct value of &lt;script type=&quot;math/tex&quot;&gt;1/4&lt;/script&gt; results in a factor of &lt;script type=&quot;math/tex&quot;&gt;1/16&lt;/script&gt; in this case as well.&lt;/p&gt;

&lt;p&gt;Together, since this factor of &lt;script type=&quot;math/tex&quot;&gt;1/16&lt;/script&gt; appears identically in both passes, it follows through into the fan-in and fan-out numbers, producing the constraint that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Var}\left[ W^{L} \right] = \frac{32}{n_\text{out} + n_\text{in}}&lt;/script&gt;

&lt;h3 id=&quot;summary-of-initialization-parameters&quot;&gt;Summary of Initialization Parameters&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Activation Function&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Uniform Distribution &lt;script type=&quot;math/tex&quot;&gt;[-a,a]&lt;/script&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Normal distribution&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Logistic&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;a = 4\sqrt{\frac{6}{n_\text{out} + n_\text{in}}}&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\sigma =  4\sqrt{\frac{2}{n_\text{out} + n_\text{in}}}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Hyperbolic Tangent&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;a = \sqrt{\frac{6}{n_\text{out} + n_\text{in}}}&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\sigma =  \sqrt{\frac{2}{n_\text{out} + n_\text{in}}}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ReLU&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;a = \sqrt{\frac{12}{n_\text{out} + n_\text{in}}}&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;\sigma =  \sqrt{\frac{12}{n_\text{out} + n_\text{in}}}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf&quot;&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1502.01852.pdf&quot;&gt;Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Hilbert Curves And Color Spaces</title>
   <link href="www.mnsgrg.com/2014/11/11/hilbert-curves-and-color-spaces/"/>
   <updated>2014-11-11T00:00:00-08:00</updated>
   <id>www.mnsgrg.com/2014/11/11/hilbert-curves-and-color-spaces</id>
   <content type="html">&lt;p&gt;Fractals are incredibly cool. You know you’re in for a treat when fractional
dimensions are a (meaningful) thing and things like shapes that enclose a
finite area with an infinite perimeter actually exist. A lot of fractals are
mathematically tractable, which usually means pretty pictures. This post is
basically about pretty pictures.&lt;/p&gt;

&lt;h2 id=&quot;hilbert-curves&quot;&gt;Hilbert Curves&lt;/h2&gt;
&lt;p&gt;The particular fractal I’ve been playing with was first described by David Hilbert,
an illustrious man whose name prepends a veritable litany of mathematical concepts.
The curve itself is a continuous fractal space-filling curve - it has no breaks,
it is self-similar, and it fills space.&lt;/p&gt;

&lt;p&gt;Why is it useful? Locality. Take the 2 dimensional Hilbert curve, for instance. Two points
that are nearby on the curve (measuring distance along the curve) will also be
nearby in 2-D space. That is, for two points &lt;script type=&quot;math/tex&quot;&gt;A(x_1,y_1)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;B(x_2,y_2)&lt;/script&gt; that are
distances &lt;script type=&quot;math/tex&quot;&gt;d_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;d_2&lt;/script&gt; along the curve, respectively, if &lt;script type=&quot;math/tex&quot;&gt;d_2-d_1&lt;/script&gt; is a small number,
the Euclidean distance between the points A,B &lt;script type=&quot;math/tex&quot;&gt;\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2}&lt;/script&gt; will
also be relatively small. Of course, the converse is not always true - there will
always be points that are nearby in 2-D space that, when mapped onto the curve,
are relatively far away. This is inevitable when mapping from a high-dimensional
space to a lower dimensional space. However, the Hilbert curve minimizes the number
of such pairs, which is nice.&lt;/p&gt;

&lt;h2 id=&quot;rgb-space&quot;&gt;RGB Space&lt;/h2&gt;
&lt;p&gt;How do we represent color? As a 3-tuple of values indicating the intensity of
each of the primary colors - red, green, and blue. The color of each pixel in
an image is determined by the relative values of the red, green, and blue in it -
represented as a number between 0 and 255. So, for example, the tuple (0,0,0)
represents the color black - the complete absence of each of the primary colors.
The tuple (255,255,255) represents the color white - each primary color is maxed
out. The tuple (255,0,0) represents pure red, (0,255,0) pure green, and (0,0,255)
pure blue. Intermediate values represent various mixtures of the three primary colors.
You get the idea.&lt;/p&gt;

&lt;h2 id=&quot;vanilla-rgb&quot;&gt;Vanilla RGB&lt;/h2&gt;
&lt;p&gt;Now, I’d like to create an image with every single RGB color ever. This is nothing
new - tens of hundreds of people have done it before in ways ranging from 
utterly fantastically creative to tear-inducingly boring, and I’m only doing
this because it’s intensely cool and the act of writing code that makes pretty
pictures is pretty satisfying. The first idea that pops up is to write a simple
nested for loop that matches every possible 3-tuple with element values between 0 and 255
to a 2-D coordinate, and draw the resulting image. It’s boring, yes, but it’s
easy to write, and runs quickly. Here’s the result:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/1315728/4998116/52ac4820-69a3-11e4-9874-77f3bb00d603.png&quot; alt=&quot;vanillargb&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can see how terrible the banding is - there is a green to blue transition across
the picture, with bands of pixels transitioning from “pure” green-blue colors
to redder variants. It’s a pretty picture alright, just not as pretty as can be.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/1315728/4999191/18ad0410-69ae-11e4-9124-43f57cfeb014.png&quot; alt=&quot;greenvanillargb&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can try changing up the order of the coordinates to vary the pictures. I’ve
done just that here, and you can see how the bands now transition across blue
and green, respectively.&lt;/p&gt;

&lt;h2 id=&quot;curving-through-space&quot;&gt;Curving through space&lt;/h2&gt;
&lt;p&gt;You’re probably wondering what that whole spiel about fractals and Hilbert curves
at the beginning of the post was about, if all I’m going to be doing is talk
about colorspaces and pictures. Here’s where it becomes important. In the
Vanilla version, I’d assigned 2-D coordinates to a point in RGB space more
or less arbitrarily - I flattened the 3-D coordinates into a single index, so
the point &lt;script type=&quot;math/tex&quot;&gt;(r,g,b) \rightarrow 256 \times 256 \times r + 256 \times g + b&lt;/script&gt;, 
and then unpacked that value into 2-D coordinates, so that the index i would map 
to the coordinates &lt;script type=&quot;math/tex&quot;&gt;((i \div 4096) \mod 4096, i \mod 4096)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This worked because I could treat the (r,g,b) tuple as a 3-digit base 256 number,
and also as a 2-digit base 4096 number, and &lt;script type=&quot;math/tex&quot;&gt;256 \times 256 \times 256 = 4096 \times 4096&lt;/script&gt;.
However, converting indices into 2-D coordinates in a more interesting manner results in way cooler
pictures. If, instead of unpacking the index as above, you treat the index as
distance along a Hilbert curve in two dimensions, you can map points in RGB space
to points along this curve. Here are the resulting pictures:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/1315728/4999185/0ff1ac90-69ae-11e4-8e52-16ca163a3c92.png&quot; alt=&quot;vanillargbhilbert2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-the-final-dimension&quot;&gt;3: The final dimension&lt;/h2&gt;
&lt;p&gt;Now that you’ve seen the prettifying power of Hilbert curves, it is time to use
Hilbert curves one last time to make the ultimate in color visualization techniques.
Recall the original “nice” property of Hilbert curves - locality. We’d like to
create an image that has smoother transitions into colors than the ones we made
above. The solution is simple - we change the index function. Instead of indexing
RGB space as though each point represents a 3 digit number is base 256, we determine
the index of the point by finding the distance of that point along the Hilbert curve
in 3-D that passes through all of RGB space.&lt;/p&gt;

&lt;p&gt;We then use this index as the distance of the point along a 2-dimensional Hilbert
curve to determine the coordinate of the point on the image that we are creating,
and assign that point to the RGB color corresponding to that index.&lt;/p&gt;

&lt;p&gt;Here are the results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/1315728/4989633/4b87920c-6949-11e4-9684-6ab5d75757a4.png&quot; alt=&quot;Hilbertrgb&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now THAT is pretty pretty.&lt;/p&gt;

&lt;h2 id=&quot;more&quot;&gt;More&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Hilbert_curve&quot;&gt;Wikipedia&lt;/a&gt; page is a pretty good reference
for general information about Hilbert curves.&lt;/p&gt;

&lt;p&gt;If you’re interested in the math behind the conversion between Hilbert indices and
coordinates for the n-dimensional curve, &lt;a href=&quot;https://www.cs.dal.ca/sites/default/files/technical_reports/CS-2006-07.pdf&quot;&gt;this paper&lt;/a&gt; is pretty helpful.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Natas 16 Blind Grep</title>
   <link href="www.mnsgrg.com/2014/11/05/natas-16-blind-grep/"/>
   <updated>2014-11-05T00:00:00-08:00</updated>
   <id>www.mnsgrg.com/2014/11/05/natas-16-blind-grep</id>
   <content type="html">&lt;p&gt;I’ll try and explain how I solved the 16th Natas challenge in this post. You
can find the challenge page &lt;a href=&quot;http://natas16.natas.labs.overthewire.org&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Playing around with the page shows that the the page takes input, greps the
input against a dictionary file, and returns the filtered output.&lt;/p&gt;

&lt;p&gt;For instance, entering “clock” returns a list of all the words in the file
dictionary.txt that contain the substring “clock”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/clock.png&quot; alt=&quot;clock&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What if we tried something that isn’t in the dictionary? A random input
takes you to a page with empty output. This will be important - the idea
that you can extract information from the page based on whether or not some
string is in the dictionary. We can use this to figure out the password
for the next phase.&lt;/p&gt;

&lt;p&gt;We know that the password will be contained in  the file /etc/natas_webpass/natas17
on the server. We simply need to somehow access this. The page takes our input
and executes it. This means that we can enclose any arbitrary shell command
in between &lt;code class=&quot;highlighter-rouge&quot;&gt;$()&lt;/code&gt; and have the server execute it. This will help us determine the
password, character by character.&lt;/p&gt;

&lt;p&gt;Consider the command &lt;code class=&quot;highlighter-rouge&quot;&gt;grep -E ^&quot;1&quot; /etc/natas_webpass/natas17&lt;/code&gt;
The -E option tells grep to treat the following part as a regex. The regex that
follows returns true if the line in the natas17 file starts with the string “1”.
Together, this command returns the password if it begins with the character 1,
and nothing otherwise. Cycling through all possible first characters, trying
commands like &lt;code class=&quot;highlighter-rouge&quot;&gt;grep -E ^&quot;2&quot; /etc/natas_webpass/natas17&lt;/code&gt;, 
&lt;code class=&quot;highlighter-rouge&quot;&gt;grep -E ^&quot;3&quot; /etc/natas_webpass/natas17&lt;/code&gt;, etc, for every possible first
character of the password, until we hit the right first character. When we do,
the command will return the entire password. Now we need to figure out how to
convert this into something we can detect.&lt;/p&gt;

&lt;p&gt;If we pass the string &lt;code class=&quot;highlighter-rouge&quot;&gt;$(grep -E ^&quot;1&quot; /etc/natas_webpass/natas17)test&lt;/code&gt; to the
input box in the page, what happens?
The server first  executes the command within the &lt;code class=&quot;highlighter-rouge&quot;&gt;$()&lt;/code&gt;, grepping the password
file against the regex we provide, and returning nothing if it does not match.
If the password does not begin with the character 1, nothing is returned. The
server then grep simply the string test against the dictionary file, returning
a list of words in the dictionary containing the substring test.
But suppose the first character of the password was the character 1. In this
case, the command within &lt;code class=&quot;highlighter-rouge&quot;&gt;$()&lt;/code&gt; returns the password, which is prepended to the
string test. The server then greps the string $password+test against the
dictionary. Now, the password is usually something like WaIHEacj63wnNIBROHeqi3p9t0m5nhmh,
which means that the server greps something like WaIHEacj63wnNIBROHeqi3p9t0m5nhmhtest
against the dictionary, which returns a page containing nothing.&lt;/p&gt;

&lt;p&gt;In short, we can guess the first character of the password in this manner, checking
to see if the result page is empty of not - it will be empty if the character we
guess is correct, and a list of strings containing the substring test if it is
not. By cycling through every possible number and letter, we can obtain the
first character of the password in this way.&lt;/p&gt;

&lt;p&gt;Continuing, we can use a command like &lt;code class=&quot;highlighter-rouge&quot;&gt;$(grep -E ^&quot;1a&quot; /etc/natas_webpass/natas17)test&lt;/code&gt;,
assuming that the first character of the password is 1, in order to check the second
character of the password. Here, we test to see if the second character is a.
Similarly to the previous case, the server returns an empty list if our guess for
the second character is correct, and a list of strings with substring test in
them if it is incorrect.&lt;/p&gt;

&lt;p&gt;Generalizing, assuming we’ve found the first k characters of the password, represented
by the string p, we can find the k+1’th character using by trying all possible
values of the k+1’th character x in the command &lt;code class=&quot;highlighter-rouge&quot;&gt;$(grep -E ^&quot;px&quot; /etc/natas_webpass/natas17)test&lt;/code&gt;,
where p is expanded to the password prefix we know, and x is the character we’re guessing,
and listening for the response from the server - our guess is correct if the
server return nothing, and incorrect if it returns a list of words.&lt;/p&gt;

&lt;p&gt;Continuing, we can find all the characters of the password, at which point we’ve
solved the challenge!&lt;/p&gt;

&lt;p&gt;You can find the code I used to automate the process described above &lt;a href=&quot;https://www.github.com/ManasGeorge/OverTheWire.git&quot;&gt;here&lt;/a&gt;
I hope you found the post helpful! Don’t hesitate to email me if you have any questions.&lt;/p&gt;
</content>
 </entry>
 

</feed>
