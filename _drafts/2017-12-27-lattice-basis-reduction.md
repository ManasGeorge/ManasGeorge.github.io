---
\rightlayout: post
---

## Lattices

A lattice (not to be confused with a lettuce) is a subspace of the vector space $$ \mathbb{R}^n $$; it is the closure of a set of vectors in that space under addition with integer coefficients. If that sounds like a mouthful, think of it as a set of evenly spaced grid points, like in the picture below. Given a basis of $$ n $$ linearly independent vectors $$ B = \{ b_1, b_2, \dots, b_n\} $$, the lattice $$ L $$ that they span is defined as the set $$ \left \{ \sum\limits_{i=1}^n z_i b_i \vert z_i \in \mathbb{Z} \land b_i \in B \right \} $$. 

The diagram below depicts an example of a 2-dimensional lattice with basis vectors $$ b_1 = \begin{bmatrix} 1.1 & 0.3 \end{bmatrix} $$ and $$   b_2 = \begin{bmatrix} 1.3 & 0.5 \end{bmatrix} $$. Note that the elements that make up the basis vectors themselves need not be integers. 

![lattice](/assets/lattice.png)

Each of the other lattice points in blue is generated by some linear combination of these two basis vectors. The basis vectors for a given lattice are not unique. In particular, the two vectors in green form an alternate basis for the same lattice. You will notice that the green vectors are shorter and more orthogonal than the red vectors. It turns out having shorter, more orthogonal basis vectors given an arbitrary lattice makes it much easier to work with them, and indirectly lead to solutions to interesting problems. One interesting question, then, is whether there is a well-defined procedure that takes an arbitrary lattice basis and produce a shorter basis for the same lattice.

As a concrete motivating problem, suppose we would like to find out whether a certain number $$ r $$ is algebraic; that is, if it can be expressed as one of the roots to a polynomial of degree $$ n $$. We will use the lattice reducing method discussed below (the famous LLL algorithm) to find an equation such that one of its roots is $$ r $$. The general plan of attack is to set up a lattice such that short vectors in that basis correspond to an equation that contains $$ r $$ as a root.

## The 2-Dimensional Case

It is easiest to start with the problem for two dimensions; we can readily visualize the vectors involved and rely on our intuition to guide us. Starting with the vectors $$ b_1 $$ and $$ b_2 $$, we wish to reduce them as much as possible. How do we know when we are done? As a first pre-condition, let us require that $$ \lVert b^_1 \rVert \le \lVert b^_2 \rVert $$. This imposes an ordering on the reduced basis vectors, which is nice to have. How do we know that we've reached the shortest possible vectors? Consider the operations we are allowed to perform on the basis without changing the span of the basis:

- We can exchange two basis vectors; clearly this doesn't change the span of the basis
- We can replace a basis vector by an integer linear combination of basis vectors

The first operation lets us ensure that the vectors are in increasing order of their norms. The second lets us reduce the norms of the vectors by subtracting appropriate multiples of the other basis vector. Taking inspiration from Euclid's algorithm for calculating the GCD of two numbers, we start by subtracting as many multiples of the smaller vector as possible from the larger one. Without loss of generality, assume $$ b_1 $$ is the smaller vector (we can always exchange vectors to make this the case). 

It turns out that taking the projection of the larger vector on to the smaller gives us the largest scalar multiple of the smaller vector we can subtract from the larger without increasing the norm of the resulting vector [^1]. Our first step, then is to update $$ b_2 $$ as follows:

$$ 
\begin{align*}
\mu_{21} &=   \frac{\langle b_1, b_2 \rangle}{\langle b_1, b_1 \rangle}  \\
b_2 &:= b_2 -  \text{nint} \left( \mu_{21} \right) b_1
\end{align*}
$$

Note that we do not use the projection $$ \mu_{21} $$ directly; we must round to the nearest integer so that the resulting vector remains within the lattice. Once we do this, we cannot reduce $$ b_2 $$ any further; adding or subtracting any multiple of $$ b_1 $$ from $$ b_2 $$ now results in a larger vector. If the new $$ b_2 $$ is smaller than $$ b_1 $$, we don't quite yet have the smallest possible basis. We exchange the two vectors and repeat the reduction process until it is no longer possible to reduce $$ b_2 $$. This corresponds to the case where $$  \left\vert \frac{\langle b_1, b_2 \rangle}{\langle b_1, b_1 \rangle} \right\vert < \frac{1}{2} $$, which means that the nearest integer is 0, implying that subtracting a multiple of $$ b_1 $$ from $$ b_2 $$ now can only make it bigger. At this point, we stop, ending up with vectors $$ b^*_1 $$ and $$ b^*_2 $$ that satisfy the conditions:

$$ 
\begin{align*}
\left\Vert b^*_1 \right\Vert & \le \left\Vert b^*_2 \right\Vert \\
\left\vert \frac{\langle b^*_1, b^*_2 \rangle}{\langle b^*_1, b^*_1 \rangle} \right\vert = \mu_{21} & \le \frac{1}{2}
\end{align*}
$$

We take these conditions to be the definition of a reduced basis, since we cannot further reduce vectors satisfying these conditions using span-preserving operations. The algorithm above (reduce, exchange, repeat) is due to Gauss, although it is also sometimes called Lagrange's algorithm.

## Generalizing to higher dimensions
### Gram Schmidt Orthogonalization Review
For a set of vectors $$ B = \{ b_1, b_2, \dots, b_n\} $$, we define the Gram-Schmidt orthogonalization coefficient $$ \mu_{ij} =  \frac{\langle b_i, b^*_j \rangle}{\langle b^*_j, b^*_j \rangle} $$. Here, $$ b^*_i $$ is the result of performing the Gram-Schmidt orthogonalization process on the vector $$ b_i $$, in the vector space that $$ B $$ spans. Not that $$ b^*_i $$ is generally not part of the lattice generated by $$ B $$; Gram-Schmidt transforms the basis by adding and subtracting arbitrary (in particular, not necessarily integral) multiples of the basis vectors with each other. An important exception is the first (smallest) vector in $$ B $$; we have $$ b^*_1 = b^1 $$ by definition. For the remaining vectors, we have:

$$ b^*_i = b_i - \sum_{j=1}^{i-1} \mu_{ij} b^*_j $$

The Gram Schmidt vectors are usually normalized, so that $$ \left\Vert b^*_i \right\Vert = 1 $$. We leave them un-normalized here; the norms of the vectors are arbitrary. The Gram Schmidt basis gives us a kind of ideal baseline to measure against while reducing the lattice basis; it is the shortest, most orthogonal basis we could have resulted in, unconstrained by the tyranny of integer multiples. It leads somewhat naturally to the definition of an LLL-reduced basis. In the reduction step, we can proceed as in the 2-dimensional case, generalized in the manner of the Gram Schmidt orthogonalization process. We reduce $$ b_i $$ by replacing it as follows:

$$ b_i :=  b_i - \sum_{j=1}^{i-1} \text{nint} \left(\mu_{ij} \right) b^*_j $$

## Expressing algebraic numbers
Consider $$ r = 0.222521 $$, which is just $$ \cos \left( \frac{3\pi}{7} \right) $$ rounded down to a few decimal places. Take the lattice in $$ \mathbb{R}^5 $$ spanned by the following vectors:

$$ \left\{ \left[1, 0, 0, 0, 1000000r^3 \right], \left[0, 1, 0, 0, 1000000r^2 \right], \left[0, 0, 1, 0, 1000000r \right], \left[0, 0, 0, 1, 1000000 \right] \right\} $$

If we could find the shortest basis vector in this lattice, it would be some integer combination of the above, of the form $$ \left[a, b, c, d, 1000000(ar^3 + br^2 + cr + d) \right] $$, with relatively small coefficients $$a,b,c,d$$ and $$ ar^3 + br^2 + cr + d $$ even smaller (close to zero). Applying LLL, the lattice reduction algorithm that the rest of this post will talk about, we find that the shortest basis vector is $$ 
[8,-4,-4,1,0] $$, which represents the equation $$ 8x^3 - 4x^2 - 4x + 1 $$, one of whose roots is, indeed $$  \cos \left( \frac{3\pi}{7} \right) $$. Interestingly enough, the other roots of the cubic found happen to be $$   \cos \left( \frac{5\pi}{7} \right) $$ and $$ \cos \left( \frac{\pi}{7} \right) $$.

[^1]: Consider the inner product of a potentially reduced vector; $$ \langle b_2 - c b_1, b_2 - c b_1 \rangle = \langle b_2, b_2 \rangle - 2c \langle b_1, b_2 \rangle + c^2 \langle b_1, b_1 \rangle $$. In order to minimize this expression, take the derivative of the inner product with respect to $$ c $$ and set it to zero, giving $$ -2 \langle b_1, b_2 \rangle + 2c \langle b_1, b_1 \rangle = 0 \implies c = \frac{\langle b_1, b_2 \rangle}{\langle b_1, b_1 \rangle} $$
