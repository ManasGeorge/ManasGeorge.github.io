<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
<link href="http://gmpg.org/xfn/11" rel="profile">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="content-type" content="text/html; charset=utf-8">

<!-- Enable responsiveness on mobile devices-->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

<title>
    
    Lattice Basis Reduction Part 1 &middot; Manas George
    
</title>

<!-- CSS -->
<link rel="stylesheet" href="/public/css/poole.css">
<link rel="stylesheet" href="/public/css/syntax.css">
<link rel="stylesheet" href="/public/css/hyde.css">
<link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

<!-- Icons -->
<link rel="shortcut icon" href="/public/favicon.ico">

<!-- RSS -->
<link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-111529460-1"></script>
<script>
window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-111529460-1');
    </script>

</head>


    <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
        },
        jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
        extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js",
                     "MathMenu.js","MathZoom.js","AssistiveMML.js"],
        TeX: {
            extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
            equationNumbers: {
                autoNumber: "AMS"
            },
            Macros: {
                bra: ["{\\mathinner{\\langle{#1}|}}",1],
                ket: ["{\\mathinner{|{#1}\\rangle}}",1],
                braket: ["{\\mathinner{\\langle{#1}\\rangle}}",1],
                Bra: ["{\\left\\langle#1\\right|}",1],
                Ket: ["{\\left|#1\\right\\rangle}",1]
            }
        }
    });
    </script>



  <body class="theme-base-08">

    <div class="sidebar">
<div class="container sidebar-sticky">
    <div class="sidebar-about">
        <h1>
            <a href="/">
                <div class="profile-picture">
                    <img src="/assets/face.png" width="200"></img>
                </div>
                <!-- Manas George -->
            </a>
        </h1>
        <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
        

        <a class="sidebar-nav-item" href="/">Blog</a>
        <a class="sidebar-nav-item" href="/resume">Resume</a>
        <a class="sidebar-nav-item" href="https://www.github.com/ManasGeorge">GitHub</a>
    </nav>
</div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Lattice Basis Reduction Part 1</h1>
  <span class="post-date">09 Aug 2018</span>
  <p>The first part of this series covers what a lattice is, what a basis for a given lattice is, what it means for that basis to be “reduced”, and the LLL algorithm, which gives us a powerful tool to take an arbitrary lattice basis and try to make it as small as possible, which turns out to be useful for mysterious reasons.</p>

<h2 id="lattices">Lattices</h2>

<p>A lattice (not to be confused with a lettuce) is a subspace of the vector space <script type="math/tex">\mathbb{R}^n</script>; it is the closure of a set of vectors in that space under addition with integer coefficients. If that mouthful sounds too confusing, think of it as a set of evenly spaced grid points, like in the picture below. Given a basis of <script type="math/tex">n</script> linearly independent vectors <script type="math/tex">B = \{ b_1, b_2, \dots, b_n\}</script>, the lattice <script type="math/tex">L</script> that they span is defined as the set <script type="math/tex">\left \{ \sum\limits_{i=1}^n z_i b_i \vert z_i \in \mathbb{Z} \land b_i \in B \right \}</script>.</p>

<p>The diagram below depicts an example of a 2-dimensional lattice with basis vectors <script type="math/tex">% <![CDATA[
b_1 = \begin{bmatrix} 1.1 & 0.3 \end{bmatrix} %]]></script> and <script type="math/tex">% <![CDATA[
b_2 = \begin{bmatrix} 1.3 & 0.5 \end{bmatrix} %]]></script>. Note that the elements that make up the basis vectors themselves need not be integers.</p>

<p><img src="/assets/lattice.png" alt="lattice" /></p>

<p>Each of the other lattice points in blue is generated by some linear combination of these two basis vectors. The basis vectors for a given lattice are not unique. In particular, the two vectors in green form an alternate basis for the same lattice. You will notice that the green vectors are shorter and more orthogonal than the red vectors. It turns out having shorter, more orthogonal basis vectors given an arbitrary lattice makes it much easier to work with them, and indirectly lead to solutions to interesting problems. If you were some kind of grad student doing math, then, you might wonder whether there exists a well-defined procedure that takes an arbitrary lattice basis and produce a shorter basis for the same lattice.</p>

<p>Why would anybody wonder about such a silly thing? As a concrete motivating problem, suppose we would like to find out whether a certain number <script type="math/tex">r</script> is algebraic; that is, if it can be expressed as one of the roots to a polynomial of degree <script type="math/tex">n</script>. We will use the lattice reducing method discussed below (the famous LLL algorithm) to find an equation such that one of its roots is <script type="math/tex">r</script>. The general plan of attack is to set up a lattice such that short vectors in that basis correspond to an equation that contains <script type="math/tex">r</script> as a root.</p>

<h2 id="the-2-dimensional-case">The 2-Dimensional Case</h2>

<p>It is easiest to start with the problem for two dimensions; we can readily visualize the vectors involved and rely on our intuition to guide us. Starting with the vectors <script type="math/tex">b_1</script> and <script type="math/tex">b_2</script>, we wish to reduce them as much as possible. That is to say, we’d like to make the two vectors as short as possible without changing their span. How do we know when we are done? As one condition, let us require that <script type="math/tex">\lVert b_1 \rVert \le \lVert b_2 \rVert</script>. This imposes an ordering on the reduced basis vectors, which is nice to have, because it means we can be sure that given a solution, we aren’t accidentally ignoring other possible solutions. How do we know that we’ve reached the shortest possible vectors? Consider the operations we are allowed to perform on the basis without changing the span of the basis:</p>

<ul>
  <li>We can exchange two basis vectors; clearly this doesn’t change the span of the basis</li>
  <li>We can replace a basis vector by an integer linear combination of basis vectors</li>
</ul>

<p>The first operation lets us ensure that the vectors are in increasing order of their norms. The second lets us reduce the norms of the vectors by subtracting appropriate multiples of the other basis vector. Taking inspiration from Euclid’s algorithm for calculating the GCD of two numbers, we start by subtracting as many multiples of the smaller vector as possible from the larger one. Without loss of generality, assume <script type="math/tex">b_1</script> is the smaller vector (we can always exchange vectors to make this the case).</p>

<p>It turns out that taking the projection of the larger vector on to the smaller gives us the largest scalar multiple of the smaller vector we can subtract from the larger without increasing the norm of the resulting vector <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>. Our first step, then is to update <script type="math/tex">b_2</script> as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mu_{21} &=   \frac{\langle b_1, b_2 \rangle}{\langle b_1, b_1 \rangle}  \\
b_2 &:= b_2 -  \text{nint} \left( \mu_{21} \right) b_1
\end{align*} %]]></script>

<p>Note that we do not use the projection <script type="math/tex">\mu_{21}</script> directly; we must round to the nearest integer so that the resulting vector remains within the lattice. Once we do this, we cannot reduce <script type="math/tex">b_2</script> any further; adding or subtracting any multiple of <script type="math/tex">b_1</script> from <script type="math/tex">b_2</script> now results in a larger vector. If the new <script type="math/tex">b_2</script> is smaller than <script type="math/tex">b_1</script>, we don’t quite yet have the smallest possible basis. We exchange the two vectors and repeat the reduction process until it is no longer possible to reduce <script type="math/tex">b_2</script>. This corresponds to the case where <script type="math/tex">% <![CDATA[
\left\vert \frac{\langle b_1, b_2 \rangle}{\langle b_1, b_1 \rangle} \right\vert < \frac{1}{2} %]]></script>, which means that the nearest integer is 0, implying that subtracting a multiple of <script type="math/tex">b_1</script> from <script type="math/tex">b_2</script> now can only make it bigger. At this point, we stop, ending up with vectors <script type="math/tex">b^*_1</script> and <script type="math/tex">b^*_2</script> that satisfy the conditions:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\left\Vert b^*_1 \right\Vert &\le \left\Vert b^*_2 \right\Vert \\
\left\vert \frac{\langle b^*_1, b^*_2 \rangle}{\langle b^*_1, b^*_1 \rangle} \right\vert = \mu_{21} &\le \frac{1}{2}
\end{align*} %]]></script>

<p>We take these conditions to be the definition of a reduced basis, since we cannot further reduce vectors satisfying these conditions using span-preserving operations. The algorithm above (reduce, exchange, repeat) is due to Gauss, although it is also sometimes called Lagrange’s algorithm.</p>

<h2 id="generalizing-to-higher-dimensions">Generalizing to higher dimensions</h2>
<h3 id="gram-schmidt-orthogonalization-review">Gram Schmidt Orthogonalization Review</h3>
<p>For a set of vectors <script type="math/tex">B = \{ b_1, b_2, \dots, b_n\}</script>, we define the Gram-Schmidt orthogonalization coefficient <script type="math/tex">\mu_{ij} =  \frac{\langle b_i, b^*_j \rangle}{\langle b^*_j, b^*_j \rangle}</script>. Here, <script type="math/tex">b^*_i</script> is the result of performing the Gram-Schmidt orthogonalization process on the vector <script type="math/tex">b_i</script>, in the vector space that <script type="math/tex">B</script> spans. We have <script type="math/tex">b^*_1 = b_1</script> by definition. For the remaining vectors, <script type="math/tex">b^*_i</script> is the portion of <script type="math/tex">b_i</script> that captures some element of the space spanned by $B$ that the vectors before it <script type="math/tex">b_1, \cdots, b_{i-1}</script> do not capture. Translating this into concrete formulae;</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
b^*_i &= b_i - \text{projection of } b_i \text{ on to } \text{span}(b_1, \cdots, b_{i-1}) \\
&= b_i - \text{projection of } b_i \text{ on to } \text{span}(b^*_1, \cdots, b^*_{i-1}) \\
&= b_i - \sum_{j=1}^{i-1} \mu_{ij} b^*_j 
\end{align*} %]]></script>

<h3 id="reduction-step-for-lll">Reduction step for LLL</h3>
<p>The Gram Schmidt vectors are usually normalized, so that <script type="math/tex">\left\Vert b^*_i \right\Vert = 1</script>. We leave them un-normalized here; the norms of the vectors are arbitrary. The Gram Schmidt basis gives us a kind of ideal baseline to measure against while reducing the lattice basis; it is the shortest, most orthogonal basis we could have found, unconstrained by the tyranny of integer multiples. It leads somewhat naturally to the definition of an LLL-reduced basis. In the reduction step, we can proceed as in the 2-dimensional case, generalized in the manner of the Gram Schmidt orthogonalization process. We reduce <script type="math/tex">b_i</script> by replacing it as follows:</p>

<script type="math/tex; mode=display">b_i :=  b_i - \sum_{j=1}^{i-1} \text{nint} \left(\mu_{ij} \right) b^*_j</script>

<p>Where <script type="math/tex">\text{nint}</script> is just the nearest integer function. Now that we have a way to reduce the lengths of our basis vectors, we can define the first condition required for an LLL-reduced basis in higher dimensions, a straightforward extension of the condition for 2 dimensions. We require that $\lvert \mu_{ij} \le \frac{1}{2} \rvert $ for $ 1 \le j &lt; i \le n $, using the same reasoning as we did for the 2 dimensional case that once this condition is met, further norm reduction within the lattice is not possible.</p>

<h3 id="imposing-an-ordering">Imposing an ordering</h3>
<p>In the 2 dimensional case, we had an obvious way of ordering the basis vectors, essentially comparing vector magnitudes within a plane. In the higher dimensional case, consider the problem of comparing the reduced vectors $b_i$ and $b_{i+1}$. Directly comparing their norms gets us nowhere, since in the ideal case all the reduced vectors have the same (unit) norm and we would be comparing vectors with different dimensions<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>. Instead, we must compare projections of the vectors onto some subspace in order to get a more varied measure of length that we can then use to compare the vectors. For $b_i$, we use the norm $ \lVert {b_i^*} \rVert $ as the required measure; the length of $b_i$ when it is stripped of the components that lie along the vectors <script type="math/tex">b_1, \cdots, b_{i-1}</script> that come before it. Similarly, we strip $b_{i+1}$ of its components along the same vectors <script type="math/tex">b_1, \cdots, b_{i-1}</script>, taking the length</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
& \lVert b_{i+1} - \sum_{j=1}^{i-1} \mu_{(i+1)j} b^*_j \rVert \\
&= \lVert b_{i+1} - \sum_{j=1}^{i} \mu_{(i+1)j} b^*_j + \mu_{(i+1)i} b^*_i \rVert \\
&= \lVert b^*_{i+1} + \mu_{(i+1)i} b^*_i \rVert \\
\end{align*} %]]></script>

<p>The goal here is to normalize both vectors by restricting them to the same subspace; by removing components of the same vectors, we effectively project them into the subspace $S_i$ that is the orthogonal complement of the span of the first $i-1$ vectors, $\text{ span}(b_1, \cdots, b_{i-1})$. This allows us to ensure an ordering using a meaningful measure of length, since the vectors are restricted to the same subspace and therefore have equal rank.</p>

<p>This gives us our ordering condition; we want the first vector to be at most the second vector by the measures we have devised:</p>

<script type="math/tex; mode=display">\lVert {b_i^*} \rVert \le \lVert b^*_{i+1} + \mu_{(i+1)i} b^*_i \rVert</script>

<p>The constraint here turns out to be too loose to ensure convergence in polynomial time, so in practice we want the difference between the measures to be larger. It suffices to require that the first vector is at most a small multiple $\delta$ of the second, where it turns out <sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup> that any $ 1 &lt; \delta &lt; 4 $ will do. For simplicity, we set $\delta = \frac{4}{3}$.</p>

<script type="math/tex; mode=display">\lVert {b_i^*} \rVert \le \delta \lVert b^*_{i+1} + \mu_{(i+1)i} b^*_i \rVert</script>

<h3 id="the-complete-algorithm">The complete algorithm</h3>
<p>Given the two conditions for a reduced basis, the final algorithm pops out almost trivially; starting from the given basis, apply the reduction step. After the reduction step has been performed, check consecutive reduced basis vectors to see if they satisfy the ordering constraint. If they do not, swap the two vectors so that they do. If at the end of this, the reduction constraint has been satisfied, we are done. Otherwise, we repeat the whole thing (reduction, swap if necessary, repeat) until both constraints are satisfied. The original paper <sup id="fnref:3:1"><a href="#fn:3" class="footnote">3</a></sup> proves that this procedure is guaranteed to terminate in polynomial time, resulting in a reduced basis. It also proves additional constraints on the basis, showing that it is “short” in the sense that the shortest vector in the basis is at most exponentially larger than the shortest vector in the entire lattice. The exponential bound may not seem all that impressive, but it tends to work fairly well in practice, and we will see later that it is good enough to solve other problems of interest.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Consider the inner product of a potentially reduced vector; <script type="math/tex">\langle b_2 - c b_1, b_2 - c b_1 \rangle = \langle b_2, b_2 \rangle - 2c \langle b_1, b_2 \rangle + c^2 \langle b_1, b_1 \rangle</script>. In order to minimize this expression, take the derivative of the inner product with respect to <script type="math/tex">c</script> and set it to zero, giving <script type="math/tex">-2 \langle b_1, b_2 \rangle + 2c \langle b_1, b_1 \rangle = 0 \implies c = \frac{\langle b_1, b_2 \rangle}{\langle b_1, b_1 \rangle}</script> <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Technically speaking, we would be comparing vectors that are projections into subspaces with different ranks; $ b_i^* $ is the projection of $b_i$ into the orthogonal complement of $\text{span}(b_1, \cdots, b_{i-1})$ which is of rank $n-i+1$, and $b_{i+1}^* $ is the the projection of $b_{i+1}$ into the orthogonal complement of $\text{span}(b_1, \cdots, b_{i})$ which is of rank $n-i$. This would effectively be the same as comparing vectors of different dimensions, in the sense that the second vector would be predictably biased by having an extra coordinate’s worth of freedom to vary in, affecting the norm. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p><a href="https://link.springer.com/article/10.1007%2FBF01457454">Lenstra, Arjen Klaas, Hendrik Willem Lenstra, and László Lovász. “Factoring polynomials with rational coefficients.” Mathematische Annalen 261.4 (1982): 515-534.</a> <a href="#fnref:3" class="reversefootnote">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2018/08/09/lattice-basis-reduction-part-2/">
            Lattice Basis Reduction Part 2
            <small>09 Aug 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2018/06/26/quantum-crypto-part-4/">
            Quantum Crypto Part 4
            <small>26 Jun 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2018/06/05/quantum-crypto-part-3/">
            Quantum Crypto Part 3
            <small>05 Jun 2018</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
